#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass report
\begin_preamble
\usepackage{pdflscape}
\usepackage{geometry}

\newenvironment{wide}{
\pagebreak
\newgeometry{
 top=15mm,
 inner=15mm,
 outer=15mm,
 bottom=15mm,
 headheight=15pt,
 headsep=5mm,
 footnotesep=5mm,
 footskip=10mm}
\begin{landscape}
% Use TiKZ?
% http://tex.stackexchange.com/questions/40501/using-restoregeometry-in-environment-next-page-runs-off-the-page-bottom/40503#40503
\setlength\LTcapwidth{\textwidth} % default: 4in (rather less than \textwidth...)
\setlength\LTleft{0pt}            % default: \parindent
\setlength\LTright{0pt}           % default: \fill
}{
\end{landscape}
\pagebreak
\aftergroup\restoregeometry
}

\usepackage{color}
\definecolor{updated}{rgb}{0.8,0.85,1}

\usepackage{numprint}

\providecommand{\versionortoday}{(Unknown version) - \today}

\providecommand{\recentlyupdated}{\emph\textsc\textcolor{updated}{ ~Updated!~ }}

\usepackage{longtable}
\usepackage{pgfplotstable}
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{array}
\pgfplotsset{compat=1.9}% supress warning

\newcolumntype{i}{>{\begin{pgfplotstablecoltype}[int detect]}r<{\end{pgfplotstablecoltype}}}
\newcolumntype{f}{>{\begin{pgfplotstablecoltype}[fixed zerofill]}r<{\end{pgfplotstablecoltype}}}

\newcommand{\tsvtablestyle}[1]{
\pgfplotstableset{
format=file,
col sep=tab,
multicolumn names={c},
column type=r,
int detect,
zerofill=true,
precision=3,
1000 sep={\ },
begin table=\begin{longtable},
end table=\end{longtable},
every odd row/.style={before row={\rowcolor[gray]{0.95}}},
every head row/.style={before row=\toprule,after row={
\midrule
\addlinespace[0pt]
\endhead
},
},
every last row/.style={after row={
\bottomrule
\caption{#1}}
},
}
}

\newcommand{\tsvtable}[4]{
\tsvtablestyle{#2}
\pgfplotstabletypeset[
columns={#3},#4
]{#1}
}

\newenvironment{futurework}{
\begin{center}
\begin{minipage}{.8\columnwidth}
\hspace{12pt}
\hrule
\hspace{12pt}
}{
\hspace{12pt}
\hrule
\hspace{12pt}
\end{minipage}
\end{center}
}
\end_preamble
\use_default_options true
\begin_modules
logicalmkup
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format pdf2
\output_sync 1
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_title "Master’s thesis"
\pdf_author "Joel Purra"
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Master’s thesis
\begin_inset Newline newline
\end_inset


\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
(Preliminary title)
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset

Swedes Online: You Are More Tracked Than You Think
\end_layout

\begin_layout Author
Joel Purra
\begin_inset Newline newline
\end_inset


\begin_inset CommandInset href
LatexCommand href
target "mig@joelpurra.se"
type "mailto:"

\end_inset

, joepu444
\begin_inset Newline newline
\end_inset


\begin_inset CommandInset href
LatexCommand href
target "http://joelpurra.com/"

\end_inset


\begin_inset Newline newline
\end_inset

+46-70-3521212
\end_layout

\begin_layout Date
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
versionortoday
\end_layout

\end_inset


\end_layout

\begin_layout Abstract
How many companies are tracking you online, and how much information does
 the average Swede leak while using popular .se websites? Many, and a lot
 - more than you think.
 Large organizations 
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
like A, B and C
\end_layout

\end_inset

 are able to connect the dots you leave behind during everyday usage, and
 construct a persona that reflects you from their perspective.
 Have you told your family, friends or colleagues about your gambling addiction,
 your sex toy purchases or your alcoholism? 
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Replace with different scary personal information.
\end_layout

\end_inset

 Even if you didn't tell anyone your deepest secrets, these companies might
 conclude that they can put labels on you by looking at everything you do
 online.
 And now they are selling it as hard facts behind the scenes.
\end_layout

\begin_layout Abstract
While browsing the web users are both actively and passively being tracked
 by multiple companies, for the purpose of building a persona for targeted
 advertising.
 Sometimes the data collection is visible, as in social network sites and
 questionnaires, but it's most common in the form of different kinds of
 external resources which may or may not serve a purpose other than keeping
 track of your every click.
 Tracking code is installed on web pages that have adverts as well as those
 that do not - the spread and reach of tracking across web pages and domains
 of different kinds increases the quality of the user data collected and
 inferred, making it more valuable for advertising purposes.
 With the extent of the use of trackers and other external resources largely
 unknown and ever evolving, what is already known raises privacy concerns
 - data considered personal leak without the user's knowledge or explicit
 permission and end up in privately owned databases for further distribution.
 Data collection is the new wild west, and you are the new cattle.
 
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Or Klondike and gold?
\end_layout

\end_inset


\end_layout

\begin_layout Abstract
To show the overlap between different domains, front pages of approximately
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
numprint{900}
\end_layout

\end_inset

 Swedish top sites and 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
numprint{13000}
\end_layout

\end_inset

 randomly selected .se domains were visited and their resources, including
 those dynamically loaded, recorded.
 Resources were grouped by mime type, URL protocol, domain, path, if it
 matches the domain the request originated from and compared to lists of
 known trackers.
 In this thesis I show the use of resources internal versus external to
 the entry domain, which the most common confirmed trackers are, what spread
 they have and how much the average Swedish internet user can expect to
 be tracked by visiting some of the most import and popular sites in Sweden.
\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Standard
\begin_inset FloatList table

\end_inset


\end_layout

\begin_layout Chapter
Introduction
\end_layout

\begin_layout Standard
This thesis is being written in the office of - and in collaboration with
 - .SE (The Internet Infrastructure Foundation), who graciously supported
 me with domain data and internet knowledge.
\end_layout

\begin_layout Section
Context
\end_layout

\begin_layout Standard
Part of .SE’s research efforts include continuously analyzing internet infrastruc
ture and usage in Sweden.
 Yearly reports convey the status of, for example, 
\emph on
Swedes and the internet
\emph default
 and 
\emph on
.SE Health Status
\emph default

\begin_inset CommandInset citation
LatexCommand cite
key "Lowinder:2012:healthstatus"

\end_inset

 to the public, both in Swedish
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "https://www.iis.se/lar-dig-mer/rapporter/"

\end_inset


\end_layout

\end_inset

 and English
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "https://www.iis.se/english/reports/"

\end_inset


\end_layout

\end_inset

.
 Information and statistics are also published on a separate portal, in
 collaboration with other organizations.
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "https://www.iis.se/vad-vi-gor/internetstatistik/"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The report 
\emph on
.SE Health Status
\emph default
 is based on data collected from around 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
numprint{900}
\end_layout

\end_inset

 .se domain names deemed of importance to the Swedish society as a whole,
 as well as random selection of 1% of the registered .se domain names.
 The research is focused on statistics about usage and security in DNS,
 IP, web and e-mail; the target audience is IT strategists, executives and
 directors.
 The thesis subject has been selected to be in line with the 
\emph on
.SE Health Status
\emph default
 reports; results may be included in future reports, and code reused by
 both those reports and potentially other tools like .SE's public tool 
\emph on
Domain Check
\emph default
.
 
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Add link to Domain Check.
\end_layout

\end_inset

 Data for the reports is analyzed and summarized by Anne-Marie Eklund Löwinder,
 a world-renown DNS and security expert
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "https://www.iis.se/bloggare/anne-marie/"

\end_inset


\end_layout

\end_inset

, while the technical aspects and tools are under the supervision of Patrik
 Wallström, a well known DNSSEC expert and free and open source software
 advocate
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "https://www.iis.se/bloggare/pawal/"

\end_inset


\end_layout

\end_inset

.
\end_layout

\begin_layout Section
People
\end_layout

\begin_layout Description
Student Joel Purra, master student for a Master of Science in Information
 Technology and Engineering, Linköping University, Sweden.
\end_layout

\begin_layout Description
University
\begin_inset space ~
\end_inset

examiner Niklas Carlsson, Associate Professor (Swedish: docent and universitetsl
ektor) at Division for Database and Information Techniques (ADIT), Department
 of Computer and Information Science (IDA), Linköping University, Sweden.
\end_layout

\begin_layout Description
Company
\begin_inset space ~
\end_inset

supervisor Patrik Wallström, Project Manager within R&D, .SE (The Internet
 Infrastructure Foundation), Sweden.
\end_layout

\begin_layout Description
Company
\begin_inset space ~
\end_inset

supervisor Staffan Hagnell, Head of New Businesses, .SE (The Internet Infrastruct
ure Foundation), Sweden.
\end_layout

\begin_layout Section
About The Internet Infrastructure Foundation
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "https://www.iis.se/"

\end_inset


\end_layout

\end_inset

 (.SE)
\end_layout

\begin_layout Standard
.SE is also known as Stiftelsen för internetinfrastruktur (IIS).
\end_layout

\begin_layout Standard
The Internet Infrastructure Foundation is an independent organization, responsib
le for the Swedish top level domain, and working for the benefit of the
 public that promotes the positive development of the internet in Sweden.
 Their head office is in Stockholm.
 In 2012 they had 61 employees and a turnover of almost 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
numprint{120}
\end_layout

\end_inset

 MSEK.
\begin_inset CommandInset citation
LatexCommand cite
key ".SE:2013:annualreport"

\end_inset


\end_layout

\begin_layout Section
Document style
\end_layout

\begin_layout Standard
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Write more about document style.
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Cross-references
\begin_inset CommandInset label
LatexCommand label
name "sub:Cross-references"

\end_inset


\end_layout

\begin_layout Standard
Many cross-references are given as chapter or section references in parentheses
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "sub:Cross-references"

\end_inset

.
\end_layout

\begin_layout Subsection
Future work
\end_layout

\begin_layout Standard
Ideas, future improvements or research has been written both in chapter
 
\begin_inset CommandInset ref
LatexCommand vref
reference "chap:Future-work"

\end_inset

 and as comments.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{futurework}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
This type of comment denotes ideas, future improvements or research.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{futurework}
\end_layout

\end_inset


\end_layout

\begin_layout Chapter
Background
\end_layout

\begin_layout Standard
In everyday web browsing, browsers routinely access a lot of material from
 other domains or services than the one visited.
\begin_inset CommandInset citation
LatexCommand cite
key "Feldmann:2004:MEI:1028788.1028833"

\end_inset

 These external resources vary from content that the user explicitly want
 to obtain, to implicitly loaded third party services, ads, and non-visible
 resources with the sole purpose of collecting user data and statistical
 material.
\begin_inset CommandInset citation
LatexCommand cite
key "Krishnamurthy:2006:CMC:1135777.1135829"

\end_inset

 All are downloaded on behalf of the user with no or few limitations, and
 oftentimes without the user's need, understanding and explicit consent.
 These external resources can all be seen as browsing habit trackers, whose
 knowledge and power increase with any additional visits to other domains
 or services loading the same resources.
\begin_inset CommandInset citation
LatexCommand cite
key "Malandrino:2013:PAI:2517840.2517868"

\end_inset


\end_layout

\begin_layout Section
Trackers are a commercial choice
\end_layout

\begin_layout Standard
While online privacy has been in the spotlight due to recently uncovered
 mass surveillance operations, the focus has been on national government
 intelligence agencies collecting information around the globe.
 They have been able to intercept traffic data and metadata by, among several
 techniques, covertly hooking into the internet infrastructure and passively
 listening.
 In contrast, external resources are approved by and actively installed
 by site and service owners, and presented openly to users with basic technical
 skills and tools.
 Reasons can be technical, for example because distributing resources among
 systems improves performance.
 Other times it is because there are positive network effects in using a
 third-party social network to promote content and products.
 More and more commonly, allowing a tracker to be installed can also become
 a source of income - data aggregation companies pay for access to users'
 data on the right site with the right quantity and quality of visitors.
 Because these external resources are used on behalf of the service, they
 are also loaded when end-to-end encryption with HTTPS is enabled for enhanced
 privacy and security.
 This encryption by-pass gives these private trackers more information than
 possible with large-scale passive traffic interception, even when there
 is a security nullifying mixture of encrypted and unencrypted connections.
\end_layout

\begin_layout Section
What is known by trackers?
\end_layout

\begin_layout Standard
Depending on what activities a user performs online, different things can
 be inferred by trackers on sites where they are installed.
 For example, a tracker on a news site can draw conclusions about interests
 from content a user reads (or choses 
\emph on
not
\emph default
 to) by tagging articles with refined keywords and creating an interest
 graph.
\begin_inset CommandInset citation
LatexCommand cite
key "Kumar:2013:GBT:2522548.2523129"

\end_inset

 The range of taggable interests of course depend on the content of the
 news site.
 Private and sensitive information leaked to third party sites during typical
 interaction with some of the most popular sites in the world include personal
 identification (full name, date of birth, email, ip address, geolocation)
 and sensitive information (sexual orientation, religious beliefs, health
 issues).
\begin_inset CommandInset citation
LatexCommand cite
key "Malandrino:2013:PAI:2517840.2517868"

\end_inset

 Social buttons, allowing users to share links with a simple click, are
 tracking users whether they are registered, logged in or not.
\begin_inset CommandInset citation
LatexCommand cite
key "Roosendaal:2010:likethis"

\end_inset

 They are especially powerful when the user is registered and logged in,
 combining the full self-provided details of the user with their browsing
 habits - all within the bounds of the services' privacy policies agreed
 to by the user.
 Once a user has provided their personal information, it is no longer just
 the individual browser or device being tracked, but the actual person using
 it - even after logging out.
\begin_inset CommandInset citation
LatexCommand cite
key "Kontaxis:2012:PSP:2362793.2362823,Krishnamurthy:2008:CPO:1397735.1397744"

\end_inset

 This direct association, as opposed to inferred, to the person also allows
 for tracking across devices where there is an overlap of services used.
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Find a reference for cross device tracking.
\end_layout

\end_inset


\end_layout

\begin_layout Section
What is the information used for?
\end_layout

\begin_layout Standard
Publishers reserve areas of their web pages for displaying different kinds
 and sizes of advertisements alongside content.
 Ads chosen for the site may be aligned with the content but it is more
 valuable the more is known about the visitors.
 Combining and aggregating information from past visitors means that more
 information can be assumed about future visitors, on a statistical basis,
 which will define the general audience of the site.
 To generate even more revenue per displayed ad, individual users are 
\emph on
targeted
\emph default
 with personalized ads depending on their specific personal data and browsing
 history.
\begin_inset CommandInset citation
LatexCommand cite
key "Gill:2013:BPF:2504730.2504768"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Write about data aggregators selling information for other purposes than
 advertising.
\end_layout

\end_inset


\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
What kind of data can be collected by trackers, and how can they be aggregated
 both per person and per group of people?
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
How much does the average user know about external resources being trackers?
\end_layout

\end_inset


\end_layout

\begin_layout Chapter
Related work
\end_layout

\begin_layout Standard
Some research has been done surrounding ad networks, trackers and their
 spread on globally popular sites, as well as what kind of private data
 users can expect to more or less inadvertently share in the course of normal
 internet usage.
 Those papers show both some of the problems and solutions in trying to
 analyze external resources.
 The Association for Computing (ACM)
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "http://acm.org/"

\end_inset


\end_layout

\end_inset

 group SIGCOMM
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "http://sigcomm.org/"

\end_inset


\end_layout

\end_inset

 has a yearly Internet Measurement Conference (IMC)
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "http://sigcomm.org/events/imc-conference"

\end_inset


\end_layout

\end_inset

, where some papers of interest have been presented.
 The Passive and Active Measurements (PAM) Conference
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "http://pam2014.cs.unm.edu/"

\end_inset


\end_layout

\end_inset

 might also have interesting papers, as well as for example ACM's archives.
 As for individuals, one of the most connected researchers in this field
 is Balachander Krishnamurthy
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "http://www2.research.att.com/~bala/papers/"

\end_inset


\end_layout

\end_inset

, who has worked with several groups looking at privacy in both online social
 networks (OSNs) and general websites.
\end_layout

\begin_layout Standard
.SE themselves have written papers analyzing the technical state of services
 connected to .se domains.
 While they haven't concentrated on exploring the web services connected
 to these domains, they do offer some groundwork in terms of selecting and
 grouping Swedish domains as well as looking at Google Analytics coverage.
\begin_inset CommandInset citation
LatexCommand cite
key "Lowinder:2011:healthstatus,Lowinder:2012:healthstatus"

\end_inset

 .SE's Internet Fund
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "http://www.internetfonden.se/"

\end_inset


\end_layout

\end_inset

 has also funded work on discussing and defining online privacy, aimed at
 those working with or developing systems that handle personal data, often
 with some kind of internet connection.
\begin_inset CommandInset citation
LatexCommand cite
key "Bylund:2013:978-91-87379-12-3:integritet"

\end_inset


\end_layout

\begin_layout Standard
Media have made reports regarding mass surveillance, especially by the United
 States intelligence agency National Security Agency (NSA)
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "http://www.nsa.gov/"

\end_inset


\end_layout

\end_inset

, but so far few papers seem to have been written.
 There are also reports on what data private companies are collecting, in
 part by their online efforts, and how they are packaging it for resale.
 While media reports aren't academic papers, they provide an up to date
 source of information needed in explaining parts of the thesis subject.
\end_layout

\begin_layout Chapter
Expected results
\begin_inset CommandInset label
LatexCommand label
name "chap:Expected-results"

\end_inset


\end_layout

\begin_layout Standard
Previous research show that ads were used on 58% of internet's 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
numprint{500}
\end_layout

\end_inset

 most popular sites.
\begin_inset CommandInset citation
LatexCommand cite
key "Krishnamurthy:2006:CMC:1135777.1135829"

\end_inset

 Google Analytics usage in a curated list of 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
numprint{900}
\end_layout

\end_inset

 Swedish domains was 44% in 2010, 57% in 2011 and 62% 2012.
\begin_inset CommandInset citation
LatexCommand cite
key "Lowinder:2010:healthstatus,Lowinder:2011:healthstatus,Lowinder:2012:healthstatus"

\end_inset

 The assumption is that the number of external resources is at least as
 big, as they include both ads and Google Analytics.
 Technical reasons include cloud services hosting sites and services, content
 delivery networks becoming commonplace
\begin_inset CommandInset citation
LatexCommand cite
key "Krishnamurthy:2000:AFI:347319.346248,Krishnamurthy:2006:CMC:1135777.1135829"

\end_inset

 for scalable speed improvements and external service providers increasing
 their quality.
 
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Add non-technical reasons.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Sites served over HTTPS are expected to use as many external resources as
 HTTP, even though some of these external resources might not be served
 over HTTPS as well.
\end_layout

\begin_layout Standard
News sites are expected to allow more trackers than other categories, as
 their income model include third party advertisements.
\begin_inset CommandInset citation
LatexCommand cite
key "Krishnamurthy:2006:CMC:1135777.1135829"

\end_inset

 Commercial sites are expected to have more trackers than government sites.
\end_layout

\begin_layout Section
Direction and scope
\end_layout

\begin_layout Standard
Emphasis for the thesis will be on technical analysis, producing aggregate
 numbers regarding domains and external resources.
 Social aspects and privacy concerns are considered out of scope.
\end_layout

\begin_layout Standard
The thesis will primarily be written from a Swedish perspective.
 This is in part because .SE has access to the full list of Swedish .se domains,
 and part because of their previous work with the 
\emph on
.SE Health Status
\emph default
 reports.
 Focus is to analyze .se domains in the reports, as they have already been
 deemed important and results can be incorporated in future reports.
 The main non-technical grouping is also based on the same reports; government,
 media, banks, larger websites, etcetera.
\end_layout

\begin_layout Standard
One assumption is that all external resources can act as trackers, even
 for static (non-script) resources with no capabilities to dynamically survey
 the user's browser, collecting data and tracking users across domains using
 for example the 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
Referer
\end_layout

\end_inset

 HTTP header
\begin_inset CommandInset citation
LatexCommand cite
key "Krishnamurthy:2006:CMC:1135777.1135829"

\end_inset

.
 While there are lists of known trackers, used by browser privacy tools,
 they are not 100% effective.
\begin_inset CommandInset citation
LatexCommand cite
key "Malandrino:2013:PAI:2517840.2517868,Krishnamurthy:2006:CMC:1135777.1135829"

\end_inset

 The lists will instead optionally be used to emphasize those external resources
 as 
\emph on
confirmed
\emph default
 trackers.
 While cookies used for tracking have been a concern for many, they are
 not necessary in order to identify most users upon return, even uniquely
 on a global level.
\begin_inset CommandInset citation
LatexCommand cite
key "Eckersley2009unique"

\end_inset

 Cookies will not be considered to be an indicator of tracking, as it can
 be assumed that a combination of other server and client side techniques
 can achieve the same goal as a tracking cookie.
\end_layout

\begin_layout Chapter
Methodology
\end_layout

\begin_layout Standard
Based on a list of domains, the front page of each domain is downloaded
 and parsed the way a browser would.
 The URL of each requested resource will be extracted, and associated with
 the domain they were loaded from.
 This data will then be classified in a number of ways, before being boiled
 down to statistics about the entire dataset.
 These aggregates are then compared between datasets.
\end_layout

\begin_layout Section
Related projects
\end_layout

\begin_layout Subsection

\emph on
.SE Health Status
\emph default

\begin_inset CommandInset label
LatexCommand label
name "sub:.SE-Health-Status"

\end_inset


\end_layout

\begin_layout Standard
The thesis topic was chosen to align with the 
\emph on
.SE Health Status
\emph default
, and one of the sources of lists of domains.
 Results from this and other runs may be included in future reports.
\end_layout

\begin_layout Subsection
HTTP Archive
\begin_inset CommandInset label
LatexCommand label
name "sub:HTTP-Archive"

\end_inset


\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "http://httparchive.org/"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
In an effort to measure web page speed on the internet, initially developed
 in October 2010, the HTTP Archive collects HAR data and runs analyses on
 them.
 Unfortunately, their data dumps are in a custom format, not the original
 HAR files, but there are some direct comparisons to be made with their
 
\emph on
Interesting stats
\emph default

\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "http://httparchive.org/interesting.php"

\end_inset


\end_layout

\end_inset

 aggregate data.
\end_layout

\begin_layout Itemize
Pages Using Google Libraries API
\end_layout

\begin_layout Itemize
HTTPS Requests
\end_layout

\begin_layout Itemize
Total Requests per Page
\end_layout

\begin_layout Subsection
.SE Domain Check
\end_layout

\begin_layout Standard
In order to facilitate repeatable and improvable analysis, tools will be
 developed to perform the collection and aggregation steps automatically.
 .SE already has a set of tools that run monthly; integration and interoperabilit
y will smooth the process and continuous usage.
 There is also a public .SE tool to allow web site owners to test their own
 sites, 
\emph on
Domain Check
\emph default
 
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Add link to Domain Check.
\end_layout

\end_inset

, which might benefit from some of the code developed within the scope of
 this thesis.
\end_layout

\begin_layout Section
Questions
\end_layout

\begin_layout Standard
With domain and resource data in place, it will be aggregated to answer
 the following questions.
\end_layout

\begin_layout Standard
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Why are these questions important? Why were they chosen?
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Group questions by refined category?
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
What kinds of resources are there?
\end_layout

\begin_layout Itemize
How many resources are internal versus external per domain?
\end_layout

\begin_layout Itemize
What is the distribution of different kinds of resources?
\end_layout

\begin_layout Itemize
How many external resources are there, considering different levels of uniquenes
s:
\end_layout

\begin_deeper
\begin_layout Itemize
Unique URLs?
\end_layout

\begin_layout Itemize
Unique per file URL?
\end_layout

\begin_layout Itemize
Unique per folder URL?
\end_layout

\begin_layout Itemize
Unique per subdomain?
\end_layout

\begin_layout Itemize
Unique per domain?
\end_layout

\begin_layout Itemize
Unique per TLD?
\end_layout

\end_deeper
\begin_layout Itemize
On how many domains is each external resource is represented?
\end_layout

\begin_layout Itemize
How does usage of external resources differ between groups of domains?
\end_layout

\begin_layout Itemize
How to mark certain external resources as known trackers?
\end_layout

\begin_layout Itemize
What is the usage and distribution of known trackers?
\end_layout

\begin_layout Itemize
Are you as tracked using secure HTTPS as insecure HTTP?
\end_layout

\begin_layout Itemize
How do the results compare to
\end_layout

\begin_deeper
\begin_layout Itemize
Historical .se data, if readily available from earlier .SE status checks?
\end_layout

\begin_layout Itemize
Other ccTLDs?
\end_layout

\begin_layout Itemize
Commonly used gTLDs?
\end_layout

\begin_layout Itemize
Recently introduced newTLDs?
\end_layout

\end_deeper
\begin_layout Standard
Additional questions, which can be considered as bonuses
\end_layout

\begin_layout Itemize
Could any external resources actually be considered internal, despite being
 loaded from external domains?
\end_layout

\begin_layout Itemize
How to determine if a resource
\end_layout

\begin_deeper
\begin_layout Itemize
Crosses Sweden's borders in transit?
\end_layout

\begin_layout Itemize
Is handled by an organization with base or ownership outside of Sweden?
\end_layout

\end_deeper
\begin_layout Itemize
Which external resources are loaded from Sweden and abroad respectively?
\end_layout

\begin_layout Itemize
What user data could potentially be collected, and subsequently inferred?
\end_layout

\begin_layout Itemize
To what extent can the average Swedish internet user's browsing habits be
 correlated across the most commonly visited webpages?
\end_layout

\begin_layout Section
Potential problems
\end_layout

\begin_layout Itemize
Due to the dynamic nature of modern web pages, a static HTML analysis might
 not be enough.
 How can pages with dynamic script loading be analyzed?
\end_layout

\begin_layout Itemize
Script aggregation and concatenation could give misleading numbers if only
 analyzed per URL.
 Is it possible to detect which known scripts are actually running?
\end_layout

\begin_layout Itemize
Can Google Tag Manager
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "http://www.google.com/tagmanager/"

\end_inset


\end_layout

\end_inset

 scripts, which is script aggregation with asynchronous loading directed
 specifically to marketers, be analyzed to show each included service?
\end_layout

\begin_layout Itemize
Can collected data served by different services differ depending on which
 tool is used to fetch the data?
\end_layout

\begin_layout Itemize
Does content and external resources vary between requests? Is it time dependent,
 or regenerated for each request? One example would be often-updated news
 sites or blogs, where new content is added and old replaced.
 Another would be ads, which might be loaded from different sources per
 request.
\end_layout

\begin_layout Itemize
Many of the external resources will be overlapping, and downloading them
 multiple times can be avoided by caching the file the first time in a run.
 Would keeping a local cache of recently requested URLs affect the results?
\end_layout

\begin_layout Itemize
Automated downloading of webpages, especially downloading several in short
 succession, can be seen by site and service owners as disruptive by using
 system resources and skewing statistical data.
 Traversing different pages on a single website can also be detected by
 looking at for example navigational patterns.
\begin_inset CommandInset citation
LatexCommand cite
key "Tan:2002:DWR:593432.593521,Lourenco:2006:CWC:1145581.1145634"

\end_inset

 By only downloading the domain root page and associated resources this
 tool might not fall into that category of detection.
 Will automated collection done for this report be detected and hindered?
\end_layout

\begin_layout Chapter
Software
\end_layout

\begin_layout Section
Third party tools
\end_layout

\begin_layout Standard
In order to download and analyze thousands of webpages in an automated fashion,
 a set of suitable tools were sought.
 Tools that are released as free and open source software have been preferred,
 and the tools written for the thesis have also been released as such.
 Development was performed in the Mac OS X operating system, but is thought
 to be runnable on other Unix-like platforms with relative ease.
\end_layout

\begin_layout Subsection
HTTP Archive (HAR) format
\begin_inset CommandInset label
LatexCommand label
name "sub:HTTP-Archive-(HAR)-format"

\end_inset


\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "http://www.softwareishard.com/blog/har-12-spec/"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
In an effort to record and analyze network traffic as seen by individual
 browsers, the data/file format HTTP Archive (HAR) was developed.
 Browsers such as Google Chrome implement it as a complement to the network
 graph shown in the Developer Console, from where a HAR file can be exported.
 While constructed to analyze for example web performance, it also contains
 data suitable for this thesis: requested URLs and HTTP request/response
 headers such as referrer and content type.
 HAR files are based upon the JSON standard 
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Insert a link to the JSON standard.
\end_layout

\end_inset

, which is a Javascript object compatible data format commonly used to communica
te dynamic data between client side scripts in browsers and web servers.
 The most recent specification at the time of writing was HAR 1.2.
\end_layout

\begin_layout Subsection
phantomjs
\begin_inset CommandInset label
LatexCommand label
name "sub:phantomjs"

\end_inset


\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "http://phantomjs.org/"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Accessing webpages is normally done by users in a graphical browser; the
 browser downloads then displays images, executes scripts, plays videos.
 A browser is user friendly but not optimal for batch usage due to the overhead
 in constantly drawing results on screen and the lack of automation without
 external tools such as Selenium Webdriver 
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Insert link to Selenium Webdriver.
\end_layout

\end_inset

.
 While Webdriver can be used to control several kinds of browsers, such
 as Microsoft Internet Explorer, Mozilla Firefox, Google Chrome, they are
 not suitable for usage on a rack server that was not set up with 
\begin_inset Quotes eld
\end_inset

normal
\begin_inset Quotes erd
\end_inset

 browser usage in mind - that is with desktop software functionalities.
 A good alternative for such servers is phantomjs, which is built as a command
 line tool without any user interface.
 It acts like a browser internally, including rendering the webpage to a
 image buffer that isn't displayed, and is controllable through the use
 of scripts.
 One such example script included in the default installation generates
 HAR files from a webpage visit.
 phantomjs has been implemented on top of the web page renderer Webkit library
 
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Insert link to Webkit.
\end_layout

\end_inset

, also used in Apple Safari, Opera and previously Google Chrome.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{futurework}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
There are alternatives to phantomjs, but they have not been tested within
 the scope of the thesis.
 Future versions could try alternative automated browsers to verify phantomjs'
 results.
 
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Insert names and links to alternatives.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{futurework}
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
jq
\begin_inset CommandInset label
LatexCommand label
name "sub:jq"

\end_inset


\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "https://stedolan.github.io/jq/"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
While there are command line tools to transform data in for example plain
 text, CSV and XML files, tools to work with JSON files are not as prevalent.
 One such tool gaining momentum is jq, which is implemented with a domain
 specific language (DSL) suitable for extracting or transforming data.
 The DSL is based around a set of filters, similar to pipes in the Unix
 world, transforming the input and passing it on to the next stage.
 jq performs well with large datasets, as it treats data as a stream where
 each top-level object is treated separately.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{futurework}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
At the time of writing, jq is released as version 1.4.
 Support for regular expressions is planned for version 1.5, which has been
 in the making for the duration of the thesis.
 As the thesis code is run on multiple machines/systems and expected to
 deliver the same results, using standardized packages has been a part of
 ensuring that.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{futurework}
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
GNU 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
parallel
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "sub:GNU-parallel"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Tange2011a"

\end_inset


\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "https://www.gnu.org/software/parallel/"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
To parallelize task execution, GNU 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
parallel
\end_layout

\end_inset

 has been used.
 It allows an input file to be distributed among several processes/CPU cores,
 and the results to be combined into a single file.
 It helps speed up downloading of HAR files, and processing of the JSON
 data through jq, which is single threaded.
\end_layout

\begin_layout Section
Code
\end_layout

\begin_layout Standard
In order to efficiently and repeatably download and analyze web pages, special
 tools have been written.
 The source code for the respective projects have been released to the public
 under GNU General Public License version 3.0 (GPL-3.0)
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "https://www.gnu.org/licenses/gpl.html"

\end_inset


\end_layout

\end_inset

, so other projects can make use of them as well.
\end_layout

\begin_layout Subsection
har-portent
\begin_inset CommandInset label
LatexCommand label
name "sub:har-portent"

\end_inset


\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "https://github.com/joelpurra/har-portent"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Quotation
Using har-heedless to download and har-dulcify to analyze web pages in aggregate.
\end_layout

\begin_layout Standard
A set of scripts that both downloads and analyzes websites in a single run
 - see har-heedless 
\begin_inset CommandInset ref
LatexCommand eqref
reference "sub:har-heedless"

\end_inset

 and har-dulcify 
\begin_inset CommandInset ref
LatexCommand eqref
reference "sub:har-dulcify"

\end_inset

.
\end_layout

\begin_layout Subsubsection
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
domains/download-and-analyze-https-www-combos.sh <parallelism> <domainlists>
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Uses 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
domains/download-and-analyze.sh
\end_layout

\end_inset

 to download four variations of the same domains, so any differences between
 secure/insecure and www-prefixed domains can be observed.
\end_layout

\begin_layout Itemize
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
http://
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
http://www.
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
https://
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
https://www.
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
domains/download-and-analyze.sh <prefix> <parallelism> <domainlists>
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Downloads a list of domains in parallel, with automatic per-prefix/input
 file folder and log file and creation.
 It also performs automatic retries for failed domains, with an increased
 parallelism as retries are mostly expected to yield another network timeout
 or error.
\end_layout

\begin_layout Subsection
har-heedless
\begin_inset CommandInset label
LatexCommand label
name "sub:har-heedless"

\end_inset


\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "https://github.com/joelpurra/har-heedless"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Quotation
Scriptable batch downloading of webpages to generate HTTP Archive (HAR)
 files, using phantomjs.
\end_layout

\begin_layout Standard
With a simple text file with one domain name per line as input, har-heedless
 downloads all of their front pages.
 Downloads can be made either in serial or in parallel.
 The resulting HAR data is written in a folder structure per domain, with
 a timestamp in the file name.
 The script that extracts HAR data, 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
netsniff.js
\end_layout

\end_inset

, is based on example code shipped with phantomjs, but modified to be more
 stable.
\end_layout

\begin_layout Subsubsection
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
get/netsniff.js
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "sub:get/netsniff.js"

\end_inset


\end_layout

\begin_layout Standard
A modified version of 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
netsniff.js
\end_layout

\end_inset

 
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Insert link to the original netsniff.js example.
\end_layout

\end_inset

 from the phantomjs project.
\end_layout

\begin_layout Standard
Some fixes include:
\end_layout

\begin_layout Itemize
Stable, logged output when resources failed to download.
\end_layout

\begin_layout Itemize
Adding error messages as HAR comments.
\end_layout

\begin_layout Itemize
Adding a base64 encoded page screenshot as an extended field.
\end_layout

\begin_layout Itemize
Waiting a period of time after downloading the web page before generating
 output, to let asynchronous downloading, processing and rendering finish.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{futurework}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
These patches have not yet been submitted to the phantomjs project.
 They should be split up into separate parts for the convenience of the
 project maintainers.
 A complete refactoring is also an alternative, but it might be less likely
 to be accepted.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{futurework}
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
get/har.sh <url>
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Contains logic to run 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
netsniff.js
\end_layout

\end_inset

 through phantomjs.
 If phantomjs crashed or otherwise encountered an error, a fallback HAR
 file is generated with a dummy response explaining that an error occurred.
\end_layout

\begin_layout Subsubsection
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
url/single.sh <domain> [--screenshot <true|false>]
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Downloads a URL of a single domain, and takes care of writing the HAR output
 to the correct folder and file.
 If a screenshot has been requested, it is extracted (and removed) from
 the extended HAR data and written to a separate file parallel to the resulting
 HAR file.
\end_layout

\begin_layout Subsubsection
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
url/parallel.sh [parallel-processes [--screenshot <true|false>]]
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Uses GNU parallel to download multiple webpages at a time.
 The number of separate processes running is adjusted per machine, depending
 on capacity, with 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
parallel-processes
\end_layout

\end_inset

.
\end_layout

\begin_layout Subsubsection
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
domain/parallel.sh <prefix> [parallel-processes [--screenshot <true|false>]]
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Download the front pages of a list of domains, in parallel, using a specific
 prefix, such as 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
https://www.
\end_layout

\end_inset

.
 See 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
url/parallel.sh
\end_layout

\end_inset

.
\end_layout

\begin_layout Subsection
har-dulcify
\begin_inset CommandInset label
LatexCommand label
name "sub:har-dulcify"

\end_inset


\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "https://github.com/joelpurra/har-dulcify"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Quote
Extract data from HTTP Archive (HAR) files, quite possibly downloaded by
 har-heedless, for some aggregate analysis.
\end_layout

\begin_layout Standard
HAR files by themselves contain too much data, so the relevant parts need
 to be extracted.
 The extracted parts are then broken down into smaller parts that are easier
 to group and analyze, and added to the data alongside the original.
 With the expanded data in place, requests are classified by basic measures
 and matched against external datasets.
 Scripts are written to perform only a limited task and instead be chained
 together by piping the data between them.
 As the scripts generally connect in only one way, the convenience scripts
 in the 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
one-shot/
\end_layout

\end_inset

 folder are used the most.
 These convenience scripts also leave the files from partial executions,
 so the can be used for other kinds of analysis.
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Write about more scripts.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
At the time of writing, there are 59 scripts in har-dulcify.
 Here's a selection with explanations.
\end_layout

\begin_layout Subsubsection
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
one-shot/all.sh [har-folder-path]
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Runs 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
preparations.sh
\end_layout

\end_inset

, 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
data.sh
\end_layout

\end_inset

 and 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
aggregate.sh
\end_layout

\end_inset

 based on data in the 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
har-folder-path
\end_layout

\end_inset

 (defaulting to the current folder) outputting the results to the current
 folder.
 This is the true one-shot script you want to run for one off analysis.
\end_layout

\begin_layout Subsubsection
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
one-shot/preparations.sh
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Downloads, prepares and analyses third party datasets, and puts them in
 the current folder for use by subsequent scripts.
\end_layout

\begin_layout Subsubsection
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
one-shot/data.sh [har-folder-path]
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Processes all HAR files in the 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
har-folder-path
\end_layout

\end_inset

 (defaulting to the current folder), and puts the output in the same folder.
\end_layout

\begin_layout Subsubsection
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
one-shot/aggregate.sh
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Prepares data for aggregation by counting occurrences in each domain's data,
 then adds them together to a single file containing an aggregate analysis.
\end_layout

\begin_layout Subsubsection
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
domains/latest/all.sh
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Finds and lists the paths to the most recent HAR files, per domain.
\end_layout

\begin_layout Subsubsection
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
extract/request/parts.sh
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Extracts url, status, content-type (mime-type) etcetera from the originating
 domain front page request and the subsequent requests.
\end_layout

\begin_layout Subsubsection
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
extract/request/expand-parts.sh
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Keeps the original data, but also expands the url and mime-type into their
 respective parts, and adds simple grouping to status and mime-type.
\end_layout

\begin_layout Subsubsection
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
classification/basic.sh
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Add basic classifications, such as if a request is internal or external
 to the originating domain, and if the request is secured with HTTPS.
\end_layout

\begin_layout Subsubsection
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
classification/disconnect/prepare-service-list.sh
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "sub:classification/disconnect/prepare-service-list.sh"

\end_inset


\end_layout

\begin_layout Standard
Prepares Disconnect's blocking list from the original format where blocked
 domains are stored deep in the structure, to one where domains are top
 level map keys, prepared for fast lookups.
\end_layout

\begin_layout Subsubsection
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
classification/disconnect/add.sh 
\family roman
<prepared-disconnect-dataset-path>
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Matches each requested domain against Disconnect's list of domains to block,
 and adds the results to the output.
 Disconnect's original 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
service.json
\end_layout

\end_inset


\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "https://github.com/disconnectme/disconnect/raw/master/firefox/content/disconnect.safariextension/opera/chrome/data/services.json"

\end_inset


\end_layout

\end_inset

 (or 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
disconnect-plaintext.json
\end_layout

\end_inset


\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "https://services.disconnect.me/disconnect-plaintext.json"

\end_inset


\end_layout

\end_inset

) needs to be prepared through 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
classification/disconnect/prepare-service-list.sh
\end_layout

\end_inset

 before being used.
\end_layout

\begin_layout Subsubsection
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
classification/disconnect/analysis.sh
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "sub:classification/disconnect/analysis.sh"

\end_inset


\end_layout

\begin_layout Standard
Analyses Disconnect's blocking list and collects some aggregate numbers.
\end_layout

\begin_layout Subsubsection
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
classification/effective-tld/add.sh 
\family roman
<prepared-disconnect-dataset-path
\series bold
>
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Matches each requested domain against Mozilla's Public Suffix list of effective
 top level domain names, and adds the results to the output.
 The original 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
effective_tld_names.dat
\end_layout

\end_inset


\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "https://publicsuffix.org/list/effective_tld_names.dat"

\end_inset


\end_layout

\end_inset

 needs to be prepared through 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
classification/effective-tld/prepare-list.sh
\end_layout

\end_inset

 before being used.
\end_layout

\begin_layout Subsubsection
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
aggregate/prepare.sh
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Write here.
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
aggregate/prepare2.sh
\end_layout

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
Rename this script?
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Running the aggregation before analysis is currently not possible in a single
 step, as jq requires all data for the reduce step to be in memory.
 The solution is to first map the data to a suitable format, and then reduce
 them in chunks repeatedly.
\end_layout

\begin_layout Subsubsection
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
aggregate/analysis.sh
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Takes counts and lists of values, and reduces them to easy to present values,
 percentages and top lists.
\end_layout

\begin_layout Subsubsection
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
questions/google-gtm-ga-dc.sh
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Analyze the impact of Google Tag Manager on coverage for other Google services,
 specifically Google Analytics and DoubleClick.
\end_layout

\begin_layout Subsubsection
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
questions/origin-redirects.sh
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Analyze requests to see if there are redirects from the origin page initially
 requested.
 One of the most interesting things to look at is wether or not domains
 redirect to or from secure https domains.
\end_layout

\begin_layout Subsubsection
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
multiset/*.sh
\end_layout

\end_inset


\end_layout

\begin_layout Standard
A set of scripts that perform tasks on multiple datasets at once - an aggregate
 of aggregates usually.
 
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Write more about multiset scripts.
\end_layout

\end_inset


\end_layout

\begin_layout Chapter
Data sources
\end_layout

\begin_layout Section
Domains
\end_layout

\begin_layout Standard
Statistics regarding each list of domains is presented in an appendix.
\end_layout

\begin_layout Subsection

\emph on
.SE Health Status
\emph default
 domains
\end_layout

\begin_layout Standard
When .SE performs their annual 
\emph on
.SE Health Status
\emph default
 report measurements, they use an in-house curated list of domains of national
 interest.
 These domains are mostly from the .se zone and cover government, county,
 municipality, higher education, government-owned corporations, financial
 service, internet service provider (ISP), domain registrar, and media domains.
 Some domains overlap both within and between categories; domains have been
 deduplicated.
\end_layout

\begin_layout Subsection
Random .se domains
\end_layout

\begin_layout Standard
The thesis was written in collaboration with .SE, which runs the .se TLD,
 and the work focusing on the state of Swedish domains.
 Early script development was done using a sample of 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
numprint{10000}
\end_layout

\end_inset

 random domains, most often tested in groups of 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
numprint{100}
\end_layout

\end_inset

.
 The .se TLD is to be considered Sweden-centric.
\end_layout

\begin_layout Subsection
Random .dk domains
\end_layout

\begin_layout Standard
The Danish .dk TLD organization, DK Hostmaster A/S
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "https://www.dk-hostmaster.dk/"

\end_inset


\end_layout

\end_inset

, helped out with a sample of 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
numprint{10000}
\end_layout

\end_inset

 domains, chosen at random from the database of active domains in the zone.
 The .dk TLD is to be considered Denmark-centric.
\end_layout

\begin_layout Subsection
Random .com, .net domains
\end_layout

\begin_layout Standard
The maintainers of the .com, .net and .name TLDs, Verisign, allow downloading
 of the complete zone file under an agreement.
 The .com zone is the largest one by far, and the .net zone is in the top
 4.
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "http://www.keepalert.com/top-extension-ranking-july-2014-newgtlds"

\end_inset


\end_layout

\end_inset

 This allows for a random selection of sites from around the world, even
 though usage is not geographically uniform - both in terms of registrations
 and actual usage.
\end_layout

\begin_layout Subsection
Alexa Top 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
numprint{1000000}
\end_layout

\end_inset


\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "http://alexa.com/"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Alexa, owned by Amazon, is a well-known source of top sites
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "http://www.alexa.com/topsites"

\end_inset


\end_layout

\end_inset

 in the world.
 It is used in many research papers, and can be seen as the standard dataset.
 
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
List referenced research papers using it.
\end_layout

\end_inset

 Their daily 1-month average traffic rank top 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
numprint{1000000}
\end_layout

\end_inset

 list is freely available for download.
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "https://alexa.zendesk.com/hc/en-us/articles/200449834-Does-Alexa-have-a-list-of-its-top-ranked-websites-"

\end_inset


\end_layout

\end_inset

 As Alexa distinguishes between a site and a domain, some domains with several
 popular sites are listed more than once.
 URL paths have been stripped and domains have been deduplicated before
 downloading.
\end_layout

\begin_layout Subsection
Reach50
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "http://reach50.com/"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The top 50 sites in Sweden are presented by by Webmie
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "http://webmie.com/"

\end_inset


\end_layout

\end_inset

, who base their list on data from a a user panel.
 The panelists have installed an extension into their browser, tracking
 their browsing habits by automated means.
 They also have results grouped by panelists categories: women, men, age
 16-34, 35-54, 55+ but only the unfiltered top list is publicly available.
\end_layout

\begin_layout Subsection
Datasets in use
\end_layout

\begin_layout Standard
These are the final domain lists in use, including full dataset size
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "https://www.iis.se/domaner/statistik/tillvaxt/?chart=active"

\end_inset


\end_layout

\end_inset


\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "https://stats.dk-hostmaster.dk/domains/total_domains/"

\end_inset


\end_layout

\end_inset

 and selection method.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
tsvtable{domains.datasets.tsv}{Datasets in use}{}{display columns/0/.style={string
 type, column type=l}, display columns/1/.style={string type, column type=l},
 display columns/3/.style={string type, column type=l}}
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
TLD distribution
\end_layout

\begin_layout Standard
These are the top TLDs in the list of unique domains.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
tsvtable{domains.tlds.tsv}{TLDs in dataset in use}{}{display columns/2/.style={stri
ng type, column type=l},}
\end_layout

\end_inset


\end_layout

\begin_layout Section
External datasets
\end_layout

\begin_layout Subsection
Disconnect's blocking list
\begin_inset CommandInset label
LatexCommand label
name "sub:Disconnect's-blocking-list"

\end_inset


\end_layout

\begin_layout Standard
One of the most popular privacy tools is Disconnect, which blocks tracking
 sites by running as a browser plugin.
 Disconnect was started by ex-Google engineers, and seems to still have
 close ties to Google.
\end_layout

\begin_layout Standard
The Disconnect software lets users block/unblock loading resources from
 specific third-party domains.
 A list of 2149 domains is used as the basis from the blocking.
 Each entry belongs to an organization, including a link to the organizations
 webpage.
 There is also a grouping into categories, here shown with some examples.
 Worth noting is that the content category isn't blocked by default.
 
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Write about top results in datasets.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{futurework}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
There are other open source alternatives to Disconnect's blocking list,
 but they use data formats that are not as easy to parse.
 The most popular ones also do not contain information about which organization
 each blocking rule belongs to.
 See 
\begin_inset CommandInset ref
LatexCommand vref
reference "sec:Ad-and-privacy-blocking-lists"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{futurework}
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Overview
\end_layout

\begin_layout Standard
An aggregate analysis of Disconnect's blocking list, as of 2014-09-08.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
tsvtable{disconnect.categories.tsv}{Disconnect's categories}{}{display columns/0/.s
tyle={string type, column type=l},}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
tsvtable{disconnect.domains-per-organization.tsv}{Domains per organization}{}{}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
tsvtable{disconnect.organizations-in-more-than-one-category.tsv}{Organizations
 in more than one category}{}{display columns/0/.style={string type, column
 type=l},}
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Advertising
\end_layout

\begin_layout Standard
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Add technorati.com, wpp.com?
\end_layout

\end_inset


\end_layout

\begin_layout Description
overture.com Yahoo's ad network.
\end_layout

\begin_layout Description
omniture.com Adobe's ad network.
\end_layout

\begin_layout Description
amazon-adsystem.com Amazon's ad delivery network.
\end_layout

\begin_layout Subsubsection
Analytics
\end_layout

\begin_layout Description
alexa.com Amazon's web statistics service, considered an authority in web
 measurement.
\end_layout

\begin_layout Description
comscore.com Analytics service that also publishes statistics.
\end_layout

\begin_layout Description
gaug.es GitHub's analytics service.
\end_layout

\begin_layout Description
coremetrics.com Part of IBM's enterprise marketing services.
\end_layout

\begin_layout Description
newrelic.com A suite of systems monitoring and analytics software, up to
 and including browsers.
\end_layout

\begin_layout Description
nielsen.com Consumer studies.
\end_layout

\begin_layout Description
statcounter.com Web statistics tool.
\end_layout

\begin_layout Description
webtrends.com Digital marketing analytics and optimization across channels.
\end_layout

\begin_layout Subsubsection
Content
\begin_inset CommandInset label
LatexCommand label
name "sub:Disconnect-Content"

\end_inset


\end_layout

\begin_layout Standard
Sites that deliver content.
 There is a wide variety of content, from images and videos to A/B testing,
 comment and helpdesk services.
 This category is not blocked by default.
\end_layout

\begin_layout Description
brightcove.com Video hosting/monetization service.
\end_layout

\begin_layout Description
disqus.com A third party comment service.
\end_layout

\begin_layout Description
flickr.com Flickr is a photo/video hosting site, owned by Yahoo.
\end_layout

\begin_layout Description
instagram.com Facebook's photo/video sharing site.
\end_layout

\begin_layout Description
office.com Microsoft's Office suite online.
\end_layout

\begin_layout Description
optimizely.com An A/B testing service.
\end_layout

\begin_layout Description
truste.com Provides certification and tools for privacy policies in order
 to gain users' trust; “enabling businesses to safely collect and use customer
 data across web, mobile, cloud and advertising channels.” This includes
 ways to selectively opt-out from cookies features; required, functional
 or advertising.
\end_layout

\begin_layout Description
tumblr.com A popular blogging platform.
\end_layout

\begin_layout Description
uservoice.com A customer support service.
\end_layout

\begin_layout Description
vimeo.com A video site.
\end_layout

\begin_layout Description
youtube.com Google's video site.
\end_layout

\begin_layout Subsubsection
Disconnect
\end_layout

\begin_layout Standard
A special category for Facebook, Google and Twitter.
 It seems to initially have been designed to block their respective like/+1/twee
t buttons, but also contains other known tracking domains belonging to the
 same organization.
\end_layout

\begin_layout Standard
It's worth noting that adwords.google.com, doubleclick.net, admob.com and several
 other of Google's ad networks, are listed here.
\end_layout

\begin_layout Subsubsection
Social
\end_layout

\begin_layout Standard
Site with an emphasis on social aspects.
 They often have buttons to vote for, recommend or share with others.
\end_layout

\begin_layout Description
addthis.com A link sharing service aggregator.
\end_layout

\begin_layout Description
digg.com News aggregator.
\end_layout

\begin_layout Description
linkedin.com Professional social network.
\end_layout

\begin_layout Description
reddit.com Social new and link sharing, and discussion.
\end_layout

\begin_layout Subsection
Public suffix list
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "https://publicsuffix.org/"

\end_inset


\end_layout

\end_inset


\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "https://en.wikipedia.org/wiki/Public_Suffix_List"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
In the domain name system, it is not always obvious what parts of a domain
 name are a public suffix and which are open for registration by Internet
 users.
 The main example is 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
example.co.uk
\end_layout

\end_inset

, where the public suffix 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
co.uk
\end_layout

\end_inset

 is to different from the TLD 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
uk
\end_layout

\end_inset

.
 Because HTTP cookies are based on domains names, it is important to browser
 vendors to be able to recognize which parts are public suffixes to be able
 to protect users against supercookies
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "https://en.wikipedia.org/wiki/HTTP_cookie#Supercookie"

\end_inset


\end_layout

\end_inset

; cookies which are scoped to a public suffix, and therefore readable across
 all web sites under that public suffix.
 The same dataset is also useful for grouping domains without improperly
 counting 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
example.co.uk
\end_layout

\end_inset

 as a 
\emph on
user-owned subdomain
\emph default
 of 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
co.uk
\end_layout

\end_inset

, which would then render 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
co.uk
\end_layout

\end_inset

 as the most popular domain under the 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
uk
\end_layout

\end_inset

 TLD.
\end_layout

\begin_layout Standard
Swedish examples include second level domains 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
pp.se
\end_layout

\end_inset

 for privately owned domains and 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
tm.se
\end_layout

\end_inset

 for trademarks
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "https://www.iis.se/data/barred_domains_list.txt"

\end_inset


\end_layout

\end_inset

.
 These second level domains were more important before April 2003
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "https://en.wikipedia.org/wiki/.se#Pre_2003_system"

\end_inset


\end_layout

\end_inset

, when first level domain registration rules restricted registration to
 nation-wide companies, associations and authorities.
\end_layout

\begin_layout Standard
The public suffix list specification contains an algorithm for certain wildcard
 rules that have exceptions, which this thesis has not implemented fully.
 These 10 exception were deemed insignificant, as 7 of them are Japanese
 cities grouped by geographic areas and the remaining 3 seem to belong to
 ccTLD owner organizations.
\end_layout

\begin_layout Chapter
Retrieving websites and resources
\end_layout

\begin_layout Section
Computer machines
\end_layout

\begin_layout Standard
Two computers were used to download web pages - one laptop machine and one
 server machine.
 The server is significantly more powerful than the laptop, and they downloaded
 a different number of web pages at a a time.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
tsvtable{computer-machines.tsv}{Machine specifications}{}{display columns/0/.style
={string type, column type=l}, display columns/1/.style={string type, column
 type=l}, display columns/2/.style={string type, column type=l}, }
\end_layout

\end_inset


\end_layout

\begin_layout Section
Network connection
\end_layout

\begin_layout Standard
The laptop machine was connected by ethernet to the .SE office network, which
 is shared with employees' computers.
 The server machine was connected to server co-location network, which is
 shared with other servers.
 The .SE network technicians said load was kept very low, and only a few
 percent of the dedicated 100 Mbps per location was used.
 Both locations are in Stockholm city, and should therefore be well placed
 in regard to web sites hosted in Sweden.
\end_layout

\begin_layout Section
Software considerations
\end_layout

\begin_layout Standard
To expedite an automated and repeatable process, a custom set of scripts
 were written as the project har-heedless.
 The scripts are written using standard tools, available as open source
 and on multiple platforms.
\end_layout

\begin_layout Subsection
Dynamic web pages
\end_layout

\begin_layout Standard
Previous efforts to download and analyze web pages by .SE used a static approach,
 analyzing the HTML by means of simple searches for 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
http://
\end_layout

\end_inset

 and 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
https://
\end_layout

\end_inset

 strings in HTML and CSS.
 It had proven hard to maintain, and the software project was abandoned
 before the thesis was started, but hadn't yet been replaced.
 In order to better handle the dynamic nature of modern web pages, the headless
 browser phantomjs was chosen, as it would also download and execute javascript
 - a major component in both user interfaces as well as active trackers
 and ads.
\end_layout

\begin_layout Subsection
Flash files
\end_layout

\begin_layout Standard
Flash is a scriptable proprietary cross-platform vector based web technology
 owned by Adobe.
 Several kinds of content, including video players, games and ads, use Flash
 because it has historically been better suited than javascript for in-browser
 moving graphics and video.
 Flash usage has not been considered for this thesis as the technology isn
 not available on all popular web browsing platforms, notably Apple's iPad,
 and is being phased out by HTML 5 features such as 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
<canvas>
\end_layout

\end_inset

 and 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
<video>
\end_layout

\end_inset

 elements.
\end_layout

\begin_layout Subsection
Combined javascript
\end_layout

\begin_layout Standard
A common technique for speeding up web sites is to reduce the number of
 resources the browser needs to download, by combining or concatenating
 them in different ways depending on the file format.
 Javascript is a good example where there are potential benefits
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "https://developers.google.com/speed/docs/best-practices/rtt#CombineExternalJS"

\end_inset


\end_layout

\end_inset

 since functionality often is spread across several files, especially after
 the plugin style of frameworks such as jQuery
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "https://jquery.com/"

\end_inset


\end_layout

\end_inset

 emerged.
 One concern is whether or not script concatenation on a web page would
 affect script analysis at a later stage, by reducing the number of third-party
 requests.
 While it's hard to analyze all scripts, based on their wide spread use,
 third-party scripts stay on their respective home servers as software as
 a service (SaaS) to enable faster update cycles and tracking of HTTP requests.
\end_layout

\begin_layout Subsection
Google Tag Manager
\begin_inset CommandInset label
LatexCommand label
name "sub:Google-Tag-Manager"

\end_inset


\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "https://www.google.com/tagmanager/"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
One of the concerns was Google Tag Manager (GTM), which a script aggregation
 service with asynchronous loading directed specifically to marketers.
 Google provides builtin support for their AdWords, Analytics (GA) and DoubleCli
ck (DC) services.
 While simplifying management with only one 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
<script>
\end_layout

\end_inset

 tag, each part should download separately and perform the same duties,
 including “calling home” to the usual addresses.
 In order to confirm this, a query was run on one of the datasets.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
tsvtable{google-tag-manager.tsv}{Google Tag Manager versus Google Analytics
 and DoubleClick, dataset	2014-07-25 100k server}{}{display columns/0/.style={str
ing type, column type=l}, display columns/2/.style={string type, column type=r}}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The numbers point to every domain that uses Google Tag Manager uses at least
 one of Google Analytics and DoubleClick, and will therefore not obscure
 information regarding which services are called when further analyzed.
\end_layout

\begin_layout Subsection
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
robots.txt
\end_layout

\end_inset

 and 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
<meta name="robots" />
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Automated web spider/bot software can bring a significant load to a web
 server, as the spidering speed can exceed user browsing speed by far -
 a single web spider can potentially request thousands of pages in the span
 of minutes, effectively being a denial of service attack 
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Insert link to DoS information.
\end_layout

\end_inset

 on an underpowered server.
 Some sites also have information considered sensitive to being available
 for spiders, or even for certain (kinds) of spiders such as image spiders.
 The choice to not serve spiders can stem from technical reasons (bandwidth,
 server load), to privacy (don't allow information to be indexed in search
 engines) and business (don't allow data to be collected and aggregated).
 In order to instruct web spiders not to get certain kind of material, there
 are two basic mechanisms - the special 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
robots.txt
\end_layout

\end_inset

 file and the HTML header tag
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
<meta name="robots" />
\end_layout

\end_inset

.
 Both can contain instructions for certain bots not to index certain paths
 or pages, or not to follow further links stemming from a page.
\end_layout

\begin_layout Standard
Commercial software from search engines, information collectors and other
 software vendors take these explicit wishes from webmasters into consideration,
 but har-heedless has not.
 While it is automated software, it is not spidering software requesting
 many pages, following links to explore the site - it only accesses the
 front page of a domain, and resources explicitly requested by that one
 page.
 Information is not retrieved for indexing as such, as only HTTP request
 metadata is recorded.
 While some information requested to be not indexed might end up in the
 screenshots, they are kept in a format hard to re-parse for machines as
 non-public thesis data used for verification.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{futurework}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Future versions of har-heedless might implement logic that checks 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
robots.txt
\end_layout

\end_inset

 in a domain list preprocessing step, to determine wether or not the request
 should be made.
 The same list can be used to filter further resource requests made by phantomjs
, perhaps considering also other domains' 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
robot.txt
\end_layout

\end_inset

 files.
 The tag 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
<meta name="robots" />
\end_layout

\end_inset

 can also be respected, perhaps in terms of not saving screenshots for 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
noindex
\end_layout

\end_inset

 values.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{futurework}
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Parallelizing downloads
\end_layout

\begin_layout Standard
A lot of the time spent downloading a web page is spent waiting for network
 resources, especially during timeouts during retrieval.
 To speed up downloading large amounts of web pages, parallelizing was employed
 using simple scripting techniques, starting multiple processes at once
 in batches, waiting for each batch to finish before starting the next.
 This was deemed inefficient, as the download and rendering speed of the
 slowest web page would be a bottle neck.
 In a later script versions, GNU 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
parallel
\end_layout

\end_inset

 was used, with a setting to not start more jobs if the current CPU/system
 load was too high.
\end_layout

\begin_layout Standard
During initial parallelizing tests, the laptop machine was shown to be able
 to handle 100 domain downloads in parallel, but this was later scaled down
 to ensure system overload would not affect results, at the cost of significantl
y longer download times.
\end_layout

\begin_layout Subsection
Screen size
\end_layout

\begin_layout Standard
When phantomjs is running, it emulates having a browser window by keeping
 an internal graphics buffer.
 Even if web page isn't rendered and shown on screen, it still has a screen
 size, which affects layout.
 With a bit of javascript or responsive CSS, the screen size can affect
 downloaded resources.
 Javascript can be used to delay download of images and other resources
 that are 
\emph on
below the fold
\emph default
, meaning outside of the initial view the user has without scrolling in
 any direction, as a page speed improvement.
 Responsive CSS adapts the page style to the screen size in order to increase
 usability for for example mobile users, and might optimize the quality
 of downloaded images to match the screen size.
\end_layout

\begin_layout Standard
By default, phantomjs uses the physical computer's primary screen size.
 In order to reduce differences between the laptop and server machines,
 a fixed emulated window size of 1024x768 pixels was chosen.
 The basis for this is that 1024x768 pixel resolution screens have been
 the recommended screen size to design for
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "http://www.nngroup.com/articles/screen-resolution-and-page-layout/"

\end_inset


\end_layout

\end_inset

 for a long time
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "http://www.nngroup.com/articles/computer-screens-getting-bigger/"

\end_inset


\end_layout

\end_inset

, and it still is a common screen size
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "http://gs.statcounter.com/#resolution-ww-monthly-201307-201407"

\end_inset


\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
Scripted browser scrolling (vertically) through the page has not been performed,
 thus javascript scrolling events triggering for example downloading of
 images below the fold are not guaranteed.
 
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Check if all images are automatically downloaded by phantomjs, as if the
 page had been scrolled, as they seem present in screenshots.
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Screenshots
\end_layout

\begin_layout Standard
In order to visually being able to confirm that web pages have been downloaded
 correctly, a PNG screenshot can optionally be taken when the page has finished
 downloading.
 There is a processing cost associated with taking screenshots; it takes
 time for phantomjs to render the internal buffer as an image, convert it
 to base64 encoding for the augmented HAR data, jq to extract the image
 data, the system to convert the data back to binary and finally write it
 to disk.
 Extra processing is also needed to remove the screenshot from the augmented
 HAR file.
\end_layout

\begin_layout Standard
The emulated browser window size also sets a limit on the screenshot size
 when rendered, but the entire browser canvas is captured.
 This means that screenshots that are saved to disk in most cases extend
 beyond the viewport size, most often vertically; this corresponds to scrolling
 through the entire page.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
tsvtable{output-filesize.tsv}{Output file size}{}{display columns/0/.style={string
 type, column type=l}, display columns/3/.style={string type, column type=r}}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The ratio of PNG to HAR data size on disk points to the compressed PNG files
 being up to 10 times the size of the uncompressed HAR files for certain
 types of pages, for example media domains in the 
\emph on
.SE Health Status
\emph default
 report dataset.
\end_layout

\begin_layout Section
System load during downloads
\end_layout

\begin_layout Standard
System load can affect the end results, if network timeouts occur during
 downloading and processing of domains' front pages.
 Apart from CPU and memory limitations, the other users of the .SE network
 should not be affected by tests.
\end_layout

\begin_layout Standard
System load on *NIX systems can be found using the 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
uptime
\end_layout

\end_inset

 command.
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "https://en.wikipedia.org/wiki/Load_(computing)"

\end_inset


\end_layout

\end_inset

 Other processes were running at the time of these tests, so the numbers
 are not exclusive to the downloading.
 The complexity of the front pages of the currently processed domains affects
 the load, as well if screenshot generations is enabled.
 Final downloads were done with screenshots enabled.
 The table below show loads for random samples in time.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
tsvtable{system-load.tsv}{System load}{}{fixed, precision=1, display columns/0/.st
yle={string type, column type=l}, display columns/1/.style={string type,
 column type=l}, display columns/2/.style={string type, column type=i}}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Test more load.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{futurework}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
System load has been shown to vary greatly based on the type of request,
 mainly differing by HTTP and HTTPS response rates.
 High failure rates mean a lot of time is spent waiting until a set time
 limit/timeout has been reached.
 Increasing the upper bound on parallelism and instead dynamically adjusting
 the number of concurrent requests emphasizing system load should decrease
 time needed to download a set of domains with a large failure rate (see
 
\begin_inset CommandInset ref
LatexCommand vref
reference "sec:Failed-versus-non-failed"

\end_inset

).
 Time limits can also be adjusted based on previous dataset results' actual
 reply timings found in HAR data, instead of setting a high and 
\begin_inset Quotes eld
\end_inset

safe
\begin_inset Quotes erd
\end_inset

 upper bound timeout, both for page timeouts and individual resources.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{futurework}
\end_layout

\end_inset


\end_layout

\begin_layout Section
Failed domains
\end_layout

\begin_layout Standard
Some web sites are not downloaded successfully, for different reasons.
 The DNS settings might not be correct, the server may be shut down, there
 might have been a temporary network timeout, there might have been a software
 error - or the server has been programmed to not respond to automated requests
 from phantomjs and similar tools.
 Unfortunately, outside of software errors, they are hard to detect without
 external analysis of connectivity.
 Each HTTP request has their HTTP status response recorded if it is available;
 absence or numbers outside the RFC2616 range (100-599) indicates failure.
 Any error output the web page itself has produced, through javascript errors
 etcetera, have also been recorded in the HAR log or individual entry/request
 comment fields.
\end_layout

\begin_layout Standard
A distinction is made between 
\emph on
failed
\emph default
 and 
\emph on
unsuccessful
\emph default
 domains - unsuccessful domains rendered a complete response with a HTTP
 status that indicated that it was not successful.
 Domains that failed have been re-downloaded; it relieved some, but not
 all, failures.
\end_layout

\begin_layout Standard
The first round of retries rendered the greatest results, and subsequent
 retries are less successful.
 This seem to point to some intermittent failures being recoverable, and
 that some domains will not respond.
\end_layout

\begin_layout Standard
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Insert re-downloading table for at least one dataset.
\end_layout

\end_inset


\end_layout

\begin_layout Chapter
Analyzing resources
\end_layout

\begin_layout Standard
After downloading HAR files, they are processed using har-dulcify, see 
\begin_inset CommandInset ref
LatexCommand vref
reference "sub:har-dulcify"

\end_inset

.
\end_layout

\begin_layout Section
Screenshots
\end_layout

\begin_layout Standard
Screenshots were mainly used for verification during development, to see
 that the pages were loaded properly.
 While they have been retained, the manual inspection necessary makes it
 infeasible as a way to verify each and every domain's result.
\end_layout

\begin_layout Section
The HAR format
\end_layout

\begin_layout Standard
The specification includes fields that have to do with for example request/respo
nse timings and data sizes.
 Those, and other fields, are not analyzed in this thesis, so the first
 step is to extract the relevant information.
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
URL The request's URL.
 Recorded whether the request is successful or not.
 Values outside of those starting with 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
http://
\end_layout

\end_inset

 or 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
https://
\end_layout

\end_inset

 are mostly ignored.
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
Status The HTTP status code
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "http://tools.ietf.org/html/rfc2616"

\end_inset


\end_layout

\end_inset

 found in the server's response.
 Defined as a 3-digit integer result, 100-599, grouped into classes by the
 first digit.
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
Mime-type The HTTP content-type header, which is the body internet media
 type (previously known as Multipurpose Internet Mail Extensions (MIME)
 type, or ) combined with optional parameters, such as character encoding.
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
Referer The URL of the page that requested the resource.
 Can be used to build a tree of requests, but is limited by the fact that
 it requires HTML 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
<frame>
\end_layout

\end_inset

 or 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
<iframe>
\end_layout

\end_inset

 to differ from the origin page.
 The word referrer was misspelled 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
referer
\end_layout

\end_inset


\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "https://en.wiktionary.org/wiki/referer"

\end_inset


\end_layout

\end_inset

 in the original proposal
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "https://tools.ietf.org/html/rfc1945"

\end_inset


\end_layout

\end_inset

.
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
Redirect The URL of the new location of the requested resource, if defined
 by a 3xx HTTP status.
\end_layout

\begin_layout Standard
These properties will be enough to see what kinds of resources are requested,
 if requests are successful and where the request is made to.
\end_layout

\begin_layout Section
Expanding parts
\end_layout

\begin_layout Subsection
URL, referer, redirect
\end_layout

\begin_layout Standard
The URL format has several components
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "https://tools.ietf.org/html/rfc3986"

\end_inset


\end_layout

\end_inset

, with interesting ones for web listed and/or split up further here.
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
Scheme In this case, 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
http
\end_layout

\end_inset

 and 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
https
\end_layout

\end_inset

 protocols have been the most interesting to look at.
 Other interesting examples that can be found in the wild include 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
ftp
\end_layout

\end_inset

 (for file downloads), 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
data
\end_layout

\end_inset

 (for resources encoded into the URI) and 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
about
\end_layout

\end_inset

 (mostly used for blank placeholder pages).
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
Domain For this thesis, the domain part has been of a lot of interest, as
 it signifies the difference between internal and external resources.
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
Port While custom ports can be used, they usually implicitly default to
 80 for HTTP and 443 for HTTPS.
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
Path The path specifies a folder or file on the server.
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
Querystring Most parameters sent back to servers are defined in an RFC compliant
 way, but there are other variants building on for example `/` as a pseudo-path
 separator.
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
Fragment The fragment is in the web context a client-only component, and
 is not to be sent back to the server as part of a request.
 The usage affects browsers' presentation, historically only by scrolling
 to a matching named element, but modern usage includes keeping browser
 state using javascript, for example following the web spider crawlable
 hash-bang syntax
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "https://developers.google.com/webmasters/ajax-crawling/docs/getting-started"

\end_inset


\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Write more about URLs.
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Status
\end_layout

\begin_layout Standard
The status is grouped into their defined groups by the first digit.
 Groups outside of the defined range 100-599 are defined as null.
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
1xx Information
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
2xx 
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
3xx Redirection
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
4xx 
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
5xx Server errors
\end_layout

\begin_layout Standard
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Fill out the proper HTTP status group headings.
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Mime-type
\end_layout

\begin_layout Standard
The mime-type is grouped by their usage, which usually is the first group
 part.
 Here is a selection of common types grouped together.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
tsvtable{mime-types.tsv}{Mime-type grouping}{}{display columns/0/.style={string
 type, column type=l}, display columns/1/.style={string type, column type=l},
 }
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Update list, use most common types.
\end_layout

\end_inset


\end_layout

\begin_layout Section
Classification
\end_layout

\begin_layout Subsection
Basic
\end_layout

\begin_layout Standard
Simple properties in the request are checked, and their valued saved as
 a classification property.
 This property is used for grouping and for further analysis.
\end_layout

\begin_layout Description
Successful
\begin_inset space ~
\end_inset

request The status of the request is 200-299 or 304.
\end_layout

\begin_layout Description
Unsuccessful
\begin_inset space ~
\end_inset

request The status is 100-199 or 300-303 or 305-599.
\end_layout

\begin_layout Description
Failed
\begin_inset space ~
\end_inset

request The log file format is incomplete or the status is null or below
 100 or above 599.
\end_layout

\begin_layout Description
Same
\begin_inset space ~
\end_inset

domain The request is to the same domain that was first visited.
\end_layout

\begin_layout Description
Subdomain The request is to a subdomain of the domain that was first visited.
\end_layout

\begin_layout Description
Superdomain The request is to a domain to which the origin domain is a subdomain.
 This basic superdomain classification is currently not checked against
 the public suffix list for invalid superdomains.
\end_layout

\begin_layout Description
Internal
\begin_inset space ~
\end_inset

domain The request is to the same domain, a subdomain or a superdomain of
 the domain that was first visited.
\end_layout

\begin_layout Description
External
\begin_inset space ~
\end_inset

domain The request is not to an internal domain.
\end_layout

\begin_layout Description
Secure
\begin_inset space ~
\end_inset

request The request is using HTTPS.
\end_layout

\begin_layout Description
Insecure
\begin_inset space ~
\end_inset

request The request is not using HTTPS.
\end_layout

\begin_layout Subsection
Disconnect.me
\end_layout

\begin_layout Standard
The URLs in the extended data contains lists of domain components.
 As the disconnect list of blocked domains is prepared for lookups by domains,
 each of the matching domains (including shorter domain components) are
 extracted with organization, organization URL and domain category.
\end_layout

\begin_layout Subsection
Public Suffix List
\end_layout

\begin_layout Standard
The public suffix list is prepared for lookups per domain component.
 Each request's domain (including shorter domain components) is checked
 against it, and any matching public suffixes are kept in an array.
\end_layout

\begin_layout Section
Analysis
\end_layout

\begin_layout Standard
An analysis, where request classifications are counted, summed and the coverage
 calculated, is performed as an automated step.
\end_layout

\begin_layout Subsection
Origin
\end_layout

\begin_layout Standard
The origin domains are grouped separately from the requests that stemmed
 from them.
\end_layout

\begin_layout Subsection
Requested URLs
\end_layout

\begin_layout Standard
All requests are represented with their domain, and other classifications.
\end_layout

\begin_layout Subsection
Distinct requested URLs
\end_layout

\begin_layout Standard
Even if two requests originating from a domain, it is only counted once.
 This gives the possibility to calculate coverage per domain later on.
\end_layout

\begin_layout Subsection
Request/domain counts
\end_layout

\begin_layout Standard
All the numbers from all domains added together.
\end_layout

\begin_layout Subsection
Request/domain coverage
\end_layout

\begin_layout Standard
The summed up counts divided by either the total request count (for requested
 URLs) or the number of domains in the current group (for distinct requested
 URLs).
 This gives a coverage percentage - either for the percentage of the number
 of requests, or domains that has the value.
\end_layout

\begin_layout Subsection
Grouping
\end_layout

\begin_layout Standard
In order to not make too broad assumptions, some grouping was performed.
 The analysis was performed the same way on each of these groups.
\end_layout

\begin_layout Subsubsection
Origin domain status
\end_layout

\begin_layout Standard
The origin domain's download status was checked, and grouped into both unfiltere
d and non-failed groups.
\end_layout

\begin_layout Subsubsection
Requested URLs
\end_layout

\begin_layout Standard
The list of requested URLs was grouped into unfiltered, internal and external
 URLs.
\end_layout

\begin_layout Subsection
Tree
\end_layout

\begin_layout Standard
A tree representation of the output after grouping.
 The origin is represents the original request and the requested URLs
\end_layout

\begin_layout Itemize
Unfiltered origin domains
\end_layout

\begin_deeper
\begin_layout Itemize
Origin
\end_layout

\begin_deeper
\begin_layout Itemize
Counts
\end_layout

\begin_layout Itemize
Coverage
\end_layout

\end_deeper
\begin_layout Itemize
Unfiltered URLs
\end_layout

\begin_deeper
\begin_layout Itemize
Requested URLs
\end_layout

\begin_deeper
\begin_layout Itemize
Counts
\end_layout

\begin_layout Itemize
Coverage
\end_layout

\end_deeper
\begin_layout Itemize
Distinct requested URLs
\end_layout

\begin_deeper
\begin_layout Itemize
Counts
\end_layout

\begin_layout Itemize
Coverage
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Internal URLs
\end_layout

\begin_deeper
\begin_layout Itemize
Requested URLs
\end_layout

\begin_deeper
\begin_layout Itemize
Counts
\end_layout

\begin_layout Itemize
Coverage
\end_layout

\end_deeper
\begin_layout Itemize
Distinct requested URLs
\end_layout

\begin_deeper
\begin_layout Itemize
Counts
\end_layout

\begin_layout Itemize
Coverage
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
External URLs
\end_layout

\begin_deeper
\begin_layout Itemize
Requested URLs
\end_layout

\begin_deeper
\begin_layout Itemize
Counts
\end_layout

\begin_layout Itemize
Coverage
\end_layout

\end_deeper
\begin_layout Itemize
Distinct requested URLs
\end_layout

\begin_deeper
\begin_layout Itemize
Counts
\end_layout

\begin_layout Itemize
Coverage
\end_layout

\end_deeper
\end_deeper
\end_deeper
\begin_layout Itemize
Non-failed origin domains
\end_layout

\begin_deeper
\begin_layout Itemize
Origin
\end_layout

\begin_deeper
\begin_layout Itemize
Counts
\end_layout

\begin_layout Itemize
Coverage
\end_layout

\end_deeper
\begin_layout Itemize
Unfiltered URLs
\end_layout

\begin_deeper
\begin_layout Itemize
Requested URLs
\end_layout

\begin_deeper
\begin_layout Itemize
Counts
\end_layout

\begin_layout Itemize
Coverage
\end_layout

\end_deeper
\begin_layout Itemize
Distinct requested URLs
\end_layout

\begin_deeper
\begin_layout Itemize
Counts
\end_layout

\begin_layout Itemize
Coverage
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Internal URLs
\end_layout

\begin_deeper
\begin_layout Itemize
Requested URLs
\end_layout

\begin_deeper
\begin_layout Itemize
Counts
\end_layout

\begin_layout Itemize
Coverage
\end_layout

\end_deeper
\begin_layout Itemize
Distinct requested URLs
\end_layout

\begin_deeper
\begin_layout Itemize
Counts
\end_layout

\begin_layout Itemize
Coverage
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
External URLs
\end_layout

\begin_deeper
\begin_layout Itemize
Requested URLs
\end_layout

\begin_deeper
\begin_layout Itemize
Counts
\end_layout

\begin_layout Itemize
Coverage
\end_layout

\end_deeper
\begin_layout Itemize
Distinct requested URLs
\end_layout

\begin_deeper
\begin_layout Itemize
Counts
\end_layout

\begin_layout Itemize
Coverage
\end_layout

\end_deeper
\end_deeper
\end_deeper
\begin_layout Section
Questions
\end_layout

\begin_layout Standard
Where the aggregate analysis isn't enough, there are custom questions.
 These questions/queries can be run against any previous intermediate step
 in the process, as they are saved to disk.
\end_layout

\begin_layout Subsection
Google Tag Manager
\end_layout

\begin_layout Standard
One of the questions posed beforehand was if Google Tag Manager would have
 an impact upon results.
 This question is answered in 
\begin_inset CommandInset ref
LatexCommand vref
reference "sub:Google-Tag-Manager"

\end_inset

 with the help of this data.
\end_layout

\begin_layout Subsection
Origins with redirects
\end_layout

\begin_layout Standard
Looking at preliminary results, a large portion of domains yielded a redirect
 as the initial response.
 In order to look at these redirects specifically, and determine if they
 redirect to an internal or external domain, a specific question was written.
\end_layout

\begin_layout Section
Multiset queries
\end_layout

\begin_layout Standard
After downloading several datasets, it is often interesting to compare them
 side by side.
 The multiset queries extract pieces of data from several datasets, and
 combine them into a single files.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Pages with wide tables start here.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{wide}
\end_layout

\end_inset


\end_layout

\begin_layout Chapter
Results
\end_layout

\begin_layout Standard
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Answer questions posed earlier.
\end_layout

\end_inset


\end_layout

\begin_layout Section
Differences between datasets
\end_layout

\begin_layout Standard
Domains lists chosen for this thesis come in three major categories - top
 lists, curated lists and random selection from zone files.
 While the top lists and curated lists are assumed to primarily contain
 sites with staff or enthusiasts to take care of them and make sure they
 are available and functioning, the domain lists randomly extracted from
 TLD zones might not.
 Results below seem to fall into groups of non-random and randomly selected
 domains - and result discussions often group them as such.
\end_layout

\begin_layout Section
Failed versus non-failed
\begin_inset CommandInset label
LatexCommand label
name "sec:Failed-versus-non-failed"

\end_inset


\end_layout

\begin_layout Standard
HAR data that doesn't have a parseable HTTP status outcome number (shown
 as 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
(null)
\end_layout

\end_inset

 in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:HTTP-status-codes"

\end_inset

) is considered a failed request.
 Looking at the resulting HAR files means that both local software errors
 and remote errors are considered failures, and while it might be technically
 possible to distinguish a software error from a remote error it has not
 been done apart from during initial development and debugging.
 In order to reduce temporary or intermittent problems, all domains that
 failed were retried up to two times.
\end_layout

\begin_layout Standard
Non-random domains have a failure rate of below 15% for HTTP, and below
 90% for HTTPS, meaning less than not more than 10% implement HTTPS.
 Random zone domains have a failure rate of above 20% for HTTP and above
 99% for HTTPS.
 The very low HTTPS adoption rates among random sites is both surprising
 and not surprising - while larger sites might have felt the pressure to
 implement them, a non-professional site owner might see it as both an unnecessa
ry technical challenge and an unnecessary additional cost.
 With public IPv4 addresses running out and legacy browsers requiring one
 per HTTPS certificate it can add an additional cost to rent one for exclusive
 use.
 Most X.509 
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Insert link to X.509.
\end_layout

\end_inset

 public key infrastructure (PKI) certificates 
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Insert link to PKI.
\end_layout

\end_inset

 costing money to buy and install as well.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
tsvtable{datasets.retries.rates.tsv}{Dataset HAR failure rates}{Dataset,Domains,Suc
cessful,Unsuccessful,Non-failed,Failed,Non-failure rate,Failure rate}{fixed,
 display columns/0/.style={string type, column type=l}, display columns/1/.style={
string type, column type=i}, display columns/2/.style={string type, column
 type=i}, display columns/3/.style={string type, column type=i}, display
 columns/4/.style={string type, column type=i}, display columns/5/.style={string
 type, column type=i}}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Update table with full result datasets.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
During analysis har-dulcify splits results into unfiltered and filtered,
 non-failed origin domains.
 Unless otherwise mentioned, further results are presented based only on
 non-failed domains in each dataset, as failed origin requests add nothing
 to the further resource analysis.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{futurework}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
A further analysis might disregard 4xx and 5xx responses as well, but current
 numbers suggest the difference would not be very large.
 A bigger improvement would be to separate placeholder pages and other domains
 which do not contain content, like parked domains and advertisement domains.
 A list used internally in .SE is available, but it has not been incorporated.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{futurework}
\end_layout

\end_inset


\end_layout

\begin_layout Section
HTTP status codes
\begin_inset CommandInset label
LatexCommand label
name "sec:HTTP-status-codes"

\end_inset


\end_layout

\begin_layout Standard
Choosing the term non-failed instead of successful when it comes to dividing
 and focusing result discussions has its basis in the HTTP standard, which
 defines a status code.
 Successful requests are generally shown with a HTTP status code of 200
 (actually the entire 2xx group), or a 304 which means that a previously
 cached (presumably successful) result is still valid.
 Many sites respond with a 3xx status, which isn't exactly successful as
 it doesn't contain actual content, but can not be considered a failure
 as it will most likely lead to another resource that is successful.
 While a status response of 4xx or 5xx shows there is a problem of some
 kind, for the purpose of this thesis a response that contains any HTTP
 status number is still considered a non-failure, as the remote system has
 responded with a proper HTTP response parseable by phantomjs and har-heedless.
 Overall, 4xx/5xx responses have been very rare; while this can be because
 of software problems, they do exist and therefore the software seems to
 work as intended.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
tsvtable{datasets.request-status.coverage.origin.sorted.tsv}{Dataset origin HTTP
 response code/group coverage}{}{fixed, display columns/0/.style={string
 type, column type=l}, display columns/1/.style={string type, column type=i}}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Update table with full result datasets.
\end_layout

\end_inset


\end_layout

\begin_layout Section
Internal versus external resources
\end_layout

\begin_layout Standard
The table shows non-failed origin domains having requests strictly to the
 same domain, subdomains or superdomains - together known as internal domains
 - or external (non-internal) domains.
 Origin domains that are not exclusively loading from either internal or
 external resources are loading from mixed domains.
 Mixing resources from both internal and external domains is the most common
 way to compose a web page for datasets not randomly chosen from zones,
 although it is quite common for random domains as well.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
tsvtable{datasets.non-failed.classification.domain-scope.coverage.sorted.tsv}{Internal
 versus external resources coverage}{}{fixed, display columns/0/.style={string
 type, column type=l}, display columns/1/.style={string type, column type=i}}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Update table with full result datasets.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
During analysis har-dulcify splits results into unfiltered, internal and
 external resources, each treated the same way in terms of classifications,
 allowing separate conclusions to be drawn.
\end_layout

\begin_layout Section
Insecure versus secure resources
\end_layout

\begin_layout Standard
Using HTTPS to secure the connection between site and site users is considered
 an effective way to avoid prying eyes on the otherwise technically quite
 open and insecure internet.
 Sites which handle sensitive information, such as e-commerce shops, online
 payment providers and of course banks often tout being secure to use -
 and they have strong financial incentives to provide a service that is
 (or at least comes across as) trustworthy.
 
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Reference for strong financial incentives for security?
\end_layout

\end_inset

 As browsers will warn users if a site secured with HTTPS loads resources
 over non-HTTPS connections, site developers will have to make sure each
 and every request is secure to avoid being labeled not trustworthy.
 This also applies to third-party services, which have to make sure to provide
 HTTPS in order to be able to continue providing services to sites making
 the switch to a fully secured experience.
\end_layout

\begin_layout Standard
One of the concerns with mixing in HTTP on an HTTPS site is that an attacker
 can use traffic sniffers to get a hold of sensitive information leaking
 out through HTTP, or man in the middle attacks on several kinds of resources
 to insert malicious code, even though the site is supposed to be protected.
 The follow table shows which to what extent sites manage to take full advantage
 of HTTPS, and to which extent they fail in requesting either internal or
 external resources.
\end_layout

\begin_layout Standard
While the technology has been around a long time, it doesn't seem as if
 very many sites actually use HTTPS on their sites.
 Even origin sites that respond to HTTPS requests seem to either redirect
 to a HTTP site, or load at least some of its resources over non-HTTPS connectio
ns.
 Typing in a HTTPS address into the browser's address bar will actually
 only give full HTTPS security on 27-58% of the domains - a number where
 the random domains surprisingly beat the non-random ones.
\end_layout

\begin_layout Standard
Why is adoption lower for top sites? As high-traffic sites they might have
 a high system load, and since HTTPS require some extra processing and data
 exchange, they might have deferred it until the security is 
\emph on
really
\emph default
 needed - like when passwords of financial information is entered.
 Strict HTTPS performance concerns were dismissed by Google engineers in
 2xxx 
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Look up year, insert link.
\end_layout

\end_inset

 - and Google has since implemented HTTPS as an alternative for most and
 the default for some services.
 
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Insert link to Google's HTTPS defaults.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Another concern is that curated domain lists seem to exhibit an even lower
 HTTPS adoption than both random and top domains - the domains have been
 selected as they are deemed important to the public in some way.
 
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Update paragraph after HTTPS for 
\emph on
.SE Health Status
\emph default
 datasets is finished.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
tsvtable{datasets.non-failed.classification.secure.coverage.sorted.tsv}{Secure
 versus insecure resources coverage}{}{fixed, display columns/0/.style={string
 type, column type=l}, display columns/1/.style={string type, column type=i}}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Update table with full result datasets.
\end_layout

\end_inset


\end_layout

\begin_layout Section
HTTP, HTTPS and redirects
\end_layout

\begin_layout Standard
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Write about HTTPS to HTTP redirects.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Insert table based on origin-redirects.sh.
\end_layout

\end_inset


\end_layout

\begin_layout Section
Content type group coverage
\end_layout

\begin_layout Standard
The difference between what can be achieved between different types of resources
 makes the distribution interesting.
 Images 
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Does SVG images allow loading external resources?
\end_layout

\end_inset

 and text (unless improperly labeled during transfer and parsed as another
 format) loaded by a browser provide no additional way to load further resources
, while html, scripts and styles do.
 While data resources can trigger downloading additional resources based
 on the logic that consumes the data, it still requires another type of
 resource present to do that.
\end_layout

\begin_layout Standard
Objects and external documents can also access additional resources, but
 the use of those types of resources has been very low in the extracted
 data.
 There might be several reasons, but the fact that the tests were run on
 a headless browser without additional plugins installed is probably the
 biggest in this case.
 An additional reason might be adoption of HTML5 and client side javascript
 instead of Flash for visual, dynamic material and animations.
 This evolution has been fueled by Apple's resistance towards supporting
 Flash on their handheld devices.
 
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Reference Apple's Flash implementation resistance.
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Origin
\end_layout

\begin_layout Standard
Practically all successful origin requests result in a html response.
 The range is 84-100% html, with the difference being seemingly misconfigured
 responses, some of which are redirects without actual content.
\end_layout

\begin_layout Subsection
Internal
\end_layout

\begin_layout Standard
Internal requests exclude the requests to the origin page.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
tsvtable{datasets.non-failed.mime-types.groups.coverage.internal.sorted.tsv}{Content
 type group coverage}{}{fixed, display columns/0/.style={string type, column
 type=l}, display columns/1/.style={string type, column type=i}}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Update table with full result datasets.
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
External
\end_layout

\begin_layout Standard
External resources from each group enjoy almost the same coverage as their
 internal counterparts.
 Among non-zone datasets scripts often reach above 90% coverage, showing
 that active and popular web pages contain a lot of external dynamic material.
 Images, while not dynamic, as well as styles and html are also popular
 to load externally.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{futurework}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
As these external files can load other files, it might be possible to build
 a hierarchy of requests.
 The easiest way is to look at resources loaded in HTML 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
<iframe>
\end_layout

\end_inset

 (or the now less popular 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
<frame>
\end_layout

\end_inset

) as they will have a HTTP 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
referer
\end_layout

\end_inset

 header set to the URL of the frame.
 Scripts and styles requesting other resources directly, without the use
 of frames, cannot be detected as a strict hierarchy.
 With the large number of requests made from different sites, URLs can be
 cleaned up (for example removing unique identifiers or looking at domain
 parts only) and connected in a graph and analyzed for similarities.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{futurework}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
tsvtable{datasets.non-failed.mime-types.groups.coverage.external.sorted.tsv}{Content
 type group coverage}{}{fixed, display columns/0/.style={string type, column
 type=l}, display columns/1/.style={string type, column type=i}}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Update table with full result datasets.
\end_layout

\end_inset


\end_layout

\begin_layout Section
Public suffix coverage
\end_layout

\begin_layout Standard
Resources served from external URLs may well come from other public suffixes;
 here they have been grouped by TLD.
 The connections between datasets and TLDs is interesting; .se datasets load
 more from .se domains than others, and the equivalent is valid for .dk datasets.
 We can also see that despite Alexa's top 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
numprint{10000}
\end_layout

\end_inset

 being an international list, nearly 19% of them use resources are loaded
 from .se domains.
 This points towards those sites being aware of the country of origin for
 the request, leading to localized content being served.
 It is also evident that the .com TLD is the most widespread for external
 resources - it beats same-TLD coverage 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
tsvtable{datasets.non-failed.public-suffix.coverage.external.sorted.tsv}{Public
 suffixes in external resources}{}{fixed, display columns/0/.style={string
 type, column type=l}, display columns/1/.style={string type, column type=i}}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Update table with full result datasets.
\end_layout

\end_inset


\end_layout

\begin_layout Section
Disconnect's blocking list matches
\end_layout

\begin_layout Subsection
Top domains
\begin_inset CommandInset label
LatexCommand label
name "sub:Top-domains"

\end_inset


\end_layout

\begin_layout Standard
A selection of domains, and their coverage across different datasets.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
tsvtable{datasets.non-failed.disconnect.domains.sorted.tsv}{Top Disconnect domain
 match coverage}{}{fixed, display columns/0/.style={string type, column type=l},
 display columns/1/.style={string type, column type=i}}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Update table with full result datasets.
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Top categories
\begin_inset CommandInset label
LatexCommand label
name "sub:Top-categories"

\end_inset


\end_layout

\begin_layout Standard
Categories and their coverage across different datasets.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
tsvtable{datasets.non-failed.disconnect.categories.sorted.tsv}{Disconnect category
 match coverage}{}{fixed, display columns/0/.style={string type, column type=l},
 display columns/1/.style={string type, column type=i}}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Update table with full result datasets.
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Top organizations
\begin_inset CommandInset label
LatexCommand label
name "sub:Top-organizations"

\end_inset


\end_layout

\begin_layout Standard
A selection of organizations, and their coverage across different datasets.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
tsvtable{datasets.non-failed.disconnect.organizations.sorted.tsv}{Top Disconnect
 organization match coverage}{}{fixed, display columns/0/.style={string type,
 column type=l}, display columns/1/.style={string type, column type=i}}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Update table with full result datasets.
\end_layout

\end_inset


\end_layout

\begin_layout Section
Non-blocked external domains
\end_layout

\begin_layout Standard
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Write about.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{wide}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Pages with wide tables end here.
\end_layout

\end_inset


\end_layout

\begin_layout Chapter
Discussion and conclusions
\end_layout

\begin_layout Standard
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Compare with expected results.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Write about the 
\emph on
Follow the Money: Understanding Economics of Online Aggregation and Advertising
\emph default
 report findings.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Write about 
\emph on
Third-party Identity Management Usage on the Web
\emph default
 report findings.
\end_layout

\end_inset


\end_layout

\begin_layout Section

\emph on
.SE Health Status
\emph default
 comparison
\end_layout

\begin_layout Standard
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Write about the 
\emph on
.SE Health Status
\emph default
 report findings.
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Google Analytics
\end_layout

\begin_layout Standard
One of the reasons this thesis subject was chosen was the inclusion of a
 Google Analytics coverage analysis in previous reports.
 The reports shows overall Google Analytics usage in the curated dataset
 of 44% 2010, 58% in 2011 and 62% in 2012.
\begin_inset CommandInset citation
LatexCommand cite
key "Lowinder:2010:healthstatus,Lowinder:2011:healthstatus,Lowinder:2012:healthstatus"

\end_inset

 Today, in 2014, usage in the category with the least coverage (financial
 services) is 58% while most are above 70% 
\begin_inset CommandInset ref
LatexCommand eqref
reference "sub:Top-domains"

\end_inset

.
 The highest coverage category (government owned corporations) is even above
 90%.
 Since Google Analytics can now be used from the DoubleClick domain as well,
 it makes more sense to look at the organization Google as a whole.
 The coverage jumps quite a bit, with most categories landing above 85%
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "sub:Top-organizations"

\end_inset

.
 
\end_layout

\begin_layout Subsection
Reachability
\end_layout

\begin_layout Standard
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Write about reachability of HTTP domains.
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
HTTPS coverage
\end_layout

\begin_layout Standard
.SE have measured HTTPS coverage among curated health status domains since
 at least 2007.
\begin_inset CommandInset citation
LatexCommand cite
key "Lowinder:2008:healthstatus,Lowinder:2009:healthstatus,Lowinder:2010:healthstatus,Lowinder:2011:healthstatus,Lowinder:2012:healthstatus,Lowinder:2013:healthstatus"

\end_inset

 The reports are a bit unclear about some numbers as measurement methodology
 and focus has shifted, but the general results seem to line up with the
 results in this thesis 
\begin_inset CommandInset ref
LatexCommand eqref
reference "sec:Failed-versus-non-failed"

\end_inset

.
 
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Verify results when all datasets have been downloaded.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
tsvtable{se.health-status.https.tsv}{.SE Health Status HTTPS coverage 2008-2013}{}{f
ixed, display columns/0/.style={string type, column type=l}, display columns/1/.st
yle={string type, column type=i}, display columns/2/.style={string type,
 column type=i}, display columns/3/.style={string type, column type=i}}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Measurements changed in 2009, so they might not be fully comparable.
 No HTTPS measurements were performed in 2012.
 
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Ask Amel about exact numbers?
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Write a comparison with own results per category?
\end_layout

\end_inset


\end_layout

\begin_layout Standard
24% of HTTPS sites redirect from HTTPS back to HTTP in 2013 - see also.
\end_layout

\begin_layout Section
Trackers which deliver content
\end_layout

\begin_layout Standard
In Disconnect's blocking list, there's a category called content 
\begin_inset CommandInset ref
LatexCommand eqref
reference "sub:Disconnect-Content"

\end_inset

.
 While all other categories are blocked by default, this one is not as it
 represents external resources deemed 
\emph on
desirable
\emph default
 to Disconnect's users.
 So while they are known tracker domains, they are allowed to pass 
\begin_inset Quotes eld
\end_inset

by popular demand.
\begin_inset Quotes erd
\end_inset

 This brings an advantage to companies that can deliver content, as they
 can just as well use content usage data as pure web bug/tracker usage data
 when analyzing patterns.
\end_layout

\begin_layout Standard
Google has several popular embeddable services in the content category,
 including Google Maps
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "https://developers.google.com/maps/documentation/embed/"

\end_inset


\end_layout

\end_inset

, Google Translate
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "https://translate.google.com/manager/website/"

\end_inset


\end_layout

\end_inset

 and least but not least YouTube
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "https://developers.google.com/youtube/player_parameters"

\end_inset


\end_layout

\end_inset

.
 Those are the visible examples, which users interact with.
 Lesser known examples include Recaptcha
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "https://developers.google.com/recaptcha/"

\end_inset


\end_layout

\end_inset

 which is an embeddable service to block/disallow web crawlers/bots access
 to web page features, Google Fonts
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "https://www.google.com/fonts"

\end_inset


\end_layout

\end_inset

 which serves modern web fonts for easy embedding, and Google Hosted Libraries
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "https://developers.google.com/speed/libraries/"

\end_inset


\end_layout

\end_inset

 which serves popular javascript libraries from Google's extensive CDN network
 instead of the local server for site speed/performance gains.
 Especially the two latter, served from the googleapis.com domain, are prevalent
 in several of the datasets - and they are usually loaded on every single
 page on a web site, and thus gain full insight on users' click paths and
 web history.
 In the .se random 100k dataset, 39% of non-failed domains make requests
 to googleapis.com.
\end_layout

\begin_layout Standard
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Extract a data table with popular content domains.
\end_layout

\end_inset


\end_layout

\begin_layout Section
Other notable results
\end_layout

\begin_layout Standard
Some results differ from my own assumptions.
\end_layout

\begin_layout Standard
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Write more about notable results.
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
HTTP versus HTTPS usage
\end_layout

\begin_layout Subsubsection
Zone domain lists
\end_layout

\begin_layout Standard
The random zone domain lists (.se, .dk, .com, .net) have download failures for
 22-26% of all domains when it comes to 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
http://
\end_layout

\end_inset

 and 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
http://www.
\end_layout

\end_inset

 variations.
\begin_inset CommandInset ref
LatexCommand eqref
reference "sec:Failed-versus-non-failed"

\end_inset

 The result is consistent with results from the 
\emph on
.SE Health Status
\emph default
, where ...
 
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Write about .se random http results.
\end_layout

\end_inset

 When it comes to 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
https://
\end_layout

\end_inset

 and 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
https://www.
\end_layout

\end_inset

 results in random zone selections are extremely low - a failure rate of
 over 99% for all of them.
 This means that unless a site explicitly redirects traffic from http to
 https, users usually don't gain any advantage typing in the extra 's' in
 https as the site won't respond.
\end_layout

\begin_layout Section
Automated, scalable data collection and repeatable analysis
\end_layout

\begin_layout Standard
One of the prerequisites for the type of analysis performed in this thesis
 was that all collection should be automated, repeatable and be able to
 handle tens of thousands of domains at a time.
 This goal has been achieved, and a specialized framework for analyzing
 web pages's HTTP requests has been built.
 While most of the code has been tailored to answer questions posed in this
 thesis, it is also built to be extendable, both in and between all data
 processing steps.
 More data can be included, additional datasets can be mixed in, separate
 questions can be written to query data from any stage in the data preparation
 or analysis.
 Tools have been written to easily download and compare separate lists of
 domains, and by default data is kept in its original downloaded form so
 that historical analysis can be performed.
\end_layout

\begin_layout Standard
It might be hard to convince other researchers to use code, as it might
 not fulfill all of their wishes at once on top of any 
\begin_inset Quotes eld
\end_inset

not invented here
\begin_inset Quotes erd
\end_inset

 mentality.
 Fortunately, the code is easy to run, and with proper documentation other
 groups should be able to at least test simple theories regarding web sites.
 Some of the lists of domains used as input are publicly available, and
 thus results can also be shared.
 This should encourage other groups, as looking at example data might spark
 interest.
\end_layout

\begin_layout Section
Contributions to other open source projects
\end_layout

\begin_layout Standard
During the development of code for this thesis, other projects have been
 utilized.
 In good open source manners, those projects should be improved when possible.
\end_layout

\begin_layout Subsection
The HAR specification
\end_layout

\begin_layout Standard
After looking at further processing of the data, some improvements might
 be suggested.
\end_layout

\begin_layout Standard
One such suggestion might be to add an absolute/resolved version of 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
response.redirectURL
\end_layout

\end_inset

, as specification 1.2 seems to be unclear wether or not it should be kept
 as-is from the HTTP 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
Location
\end_layout

\end_inset

 header or browser's 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
redirectURL
\end_layout

\end_inset

values - both of which possibly is relative.
 As subsequent HTTP requests are hard to refer to without relying either
 on exact request ordering (the executed redirect always coming exactly
 as the next entry) or at least having the URL resolved (preferably by the
 browser) before writing it to the HAR data.
 Current efforts in 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
netsniff.js
\end_layout

\end_inset

 
\begin_inset CommandInset ref
LatexCommand eqref
reference "sub:get/netsniff.js"

\end_inset

 to resolve relative URLs using a separate javascript library have proven
 inexact when it comes to matching against the browser's executed URL, differing
 for example in wether trailing slashes are kept for domain root requests
 or not.
 What would be even better, is a way to refer to the reason for the HTTP
 request, be it an HTML tag, a script call or a HTTP redirect - but that
 could to be highly implementation dependent per browser.
\end_layout

\begin_layout Subsection
phantomjs
\end_layout

\begin_layout Standard
While 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
netsniff.js
\end_layout

\end_inset

 
\begin_inset CommandInset ref
LatexCommand eqref
reference "sub:get/netsniff.js"

\end_inset

 from the phantomjs example library has been improved in several ways, patches
 have not yet been submitted.
 Since it is only an example from their side, a more developed version might
 no longer serve the same purpose - educating new users on the possibilities
 of phantomjs.
 An attempt to break the code down and separate pure bug fixes from other
 improvements might help.
 The version written for this thesis is released under the same license
 as the original, so reuse should not be a problem for those interested.
\end_layout

\begin_layout Subsection
jq
\end_layout

\begin_layout Standard
Using jq as the main program for data transformation and aggregation has
 given me a fair amount of knowledge of real world usage of the jq domain-specif
ic language (DSL).
 Bugs and inconsistencies have been reported, and input regarding for example
 code sharing through a package management system and (semantic) versioning
 has been given.
 Some of the code written has been packaged for easy reuse, and more is
 on the way.
\end_layout

\begin_layout Subsection
Disconnect
\end_layout

\begin_layout Standard
Disconnect relies heavily on their blocking list 
\begin_inset CommandInset ref
LatexCommand eqref
reference "sub:Disconnect's-blocking-list"

\end_inset

, as it's the base for both the service of blocking external resources and
 presenting statistics to the user.
 While preparing 
\begin_inset CommandInset ref
LatexCommand eqref
reference "sub:classification/disconnect/prepare-service-list.sh"

\end_inset

 and analyzing 
\begin_inset CommandInset ref
LatexCommand eqref
reference "sub:classification/disconnect/analysis.sh"

\end_inset

 the blocking list, a number of errors and inconsistencies were found.
 Unfortunately, the maintainers do not seem very active in the project,
 and even trivial data encoding errors were not patched over a month after
 submission.
 According to Disconnect's Eason Goodale in an email conversation 2014-08-13,
 the team has been concentrating on a second version of Disconnect as well
 as other projects.
 While I can submit patches through Disconnect's Github project pages, Goodale's
 reply seems to indicate they will not be accepted in a timely fashion and
 perhaps irrelevant by the time the next generation is released to the public.
\end_layout

\begin_layout Subsection
Public Suffix
\end_layout

\begin_layout Standard
A tool that parses the public suffix list from its original format to a
 JSON lookup object format has been written.
 Using that tool an inconsistency in the data was detected - the TLD .engineering
 being included twice instead of .engineer and .engineering separately.
 This had already been detected and reported by others, but it can be used
 to detect future inconsistencies in an automated manner.
\end_layout

\begin_layout Chapter
Future work
\begin_inset CommandInset label
LatexCommand label
name "chap:Future-work"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Mention further grouping, digging into the data.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Mention sharing data when using non-private lists.
\end_layout

\end_inset


\end_layout

\begin_layout Section
Creating an information website
\end_layout

\begin_layout Standard
As some of this information is hard to retrieve, analyze and process for
 individuals, especially in aggregate despite code being open source, a
 separate tool performing the work for anyone should be created.
 Apart from presenting data already collected as a part of the thesis, it
 could accept user input to analyze individual domains.
 With several domains as input, any overlap can be detected and presented
 to the user as an information sharing graph.
 One of the inspirations for this thesis was Collusion 
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Insert link to Collusion, or whatever it is called these days.
\end_layout

\end_inset

, which is a tool to dynamically display from which external domains a page
 retrieves resources right in the browser.
 A version of the same tool could be built, where instead of letting the
 user's browser retrieve sites the server would do it.
 This way a non-technical user doesn't have to 
\begin_inset Quotes eld
\end_inset

risk
\begin_inset Quotes erd
\end_inset

 anything by visiting web pages, and their relationship could be displayed
 anyways.
 It also allows for cached lookups and a grander scope, where further relationsh
ips apart from the first hand ones could be suggested.
 
\begin_inset Quotes eld
\end_inset

If you frequently visit these sites, you might also be visiting theses sites
 - click to display their relationships as well.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Standard
Over time and with user input, the dataset collected on the server would
 increase, and a historical graph relating to both results shown in this
 thesis and the relationships between sites can be created.
 This is similar to what both the HTTP Archive 
\begin_inset CommandInset ref
LatexCommand eqref
reference "sub:HTTP-Archive"

\end_inset

 is doing on a large scale but with slightly different focus, and what the
 
\emph on
.SE Health Status
\emph default
 is doing but on a less continuous basis and a shifting focus.
\end_layout

\begin_layout Section
Code documentation
\end_layout

\begin_layout Standard
With some 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
numprint{70}
\end_layout

\end_inset

 scripts written and released as open source for the thesis, the need for
 documentation has gradually increased.
 The reason for not writing proper documentation - not having direct collaborato
rs writing code - is a hinderance for future collaborators or users to get
 started.
 While code documentation has not been an explicit part of the thesis plan,
 it can be seen as an important step for future usage.
 The code is not magic in any way, but if understanding the functionality
 of a file required reading over a hundred lines of code instead of two
 or three lines of comments, it means a rather steep learning curve for
 something that is supposed to be simple.
\end_layout

\begin_layout Section
Improving domain retrieval
\end_layout

\begin_layout Subsection
Automated testing
\end_layout

\begin_layout Standard
So far all testing of har-heedless and phantomjs has been done manually.
 It has proven to be a working setup, as thesis results are based on these
 tools, but the features are to be considered fragile as there are no regression
 tests.
 Automated tests of the different levels (shell scripts, 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
netsniff.js
\end_layout

\end_inset

 
\begin_inset CommandInset ref
LatexCommand eqref
reference "sub:get/netsniff.js"

\end_inset

, screenshots, error handling) might help achieve stability in case of for
 example future improvements of phantomjs.
 Tests might include setting up a web server with test pages serving different
 kinds of content, as well as different kinds of errors.
 During mass downloading of domains phantomjs has been observed outputting
 error messages, such as failed JPEG image decoding and unspecified crashes.
 The extent of these errors have so far not been examined, as they have
 ended up being clumped together with external errors such as network or
 remote server failures.
\end_layout

\begin_layout Subsection
Investigating failed domains
\end_layout

\begin_layout Standard
There are many reasons domain retrieval could fail, but for top or curated
 domain lists the chances of the site being down are considerably lower
 than for randomly selected domains.
 Despite this, certain sites do not respond to requests from the automated
 software.
 There are several ways for a remote system to detect requests from automation
 software, with the simples one being looking at the HTTP 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
User-Agent
\end_layout

\end_inset

 string.
 As some sites respond to desktop browser requests, but not har-heedless'
 requests, it is believed they have implemented certain 
\begin_inset Quotes eld
\end_inset

protection
\begin_inset Quotes erd
\end_inset

 from this kinds of software.
 In not modifying the browser to try to avoid these measures I have tried
 to respect their will not to serve automated requests.
\end_layout

\begin_layout Subsection
Browser plugins
\end_layout

\begin_layout Standard
If possible, a set of common browser plugins could be installed into phantomjs.
 The first that comes into mind is Adobe Flash, which is sometimes used
 to display dynamic ads.
 Flash also has the ability to request resources from other domains, so
 it might affect results to not render them.
 An additional problem might be that Flash has its own cookie system, which
 used storage external to the browser.
 This brings a new set of potential problems, as Flash cookies are a big
 part of evercookies and cookie respawning.
\begin_inset CommandInset citation
LatexCommand cite
key "G.-Acar:persistent:2014aa"

\end_inset

 This means that a headless browser without persistent storage might end
 up having identifier cookies set in Flash storage, thus being easily and
 uniquely identified on subsequent visits.
 While this might not affect this thesis much, it might affect other kinds
 of research being conducted based on the same tools.
\end_layout

\begin_layout Subsection
System language
\end_layout

\begin_layout Standard
Tests were run on English language systems, without making any customizations
 to phantomjs' settings or HTTP 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
Accept-Language
\end_layout

\end_inset

 headers.
 While sites have been downloaded from around the world, localized domains
 might behave differently depending on user language.
 Google has a recommendation saying that they will prioritize TLDs specific
 to a region with a certain language (like .se and Sweden) for users sending
 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
Accept-Language
\end_layout

\end_inset

 prioritizing Swedish.
 
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Insert link to Google's language page ranking.
\end_layout

\end_inset

 This stems from them seeing that localized results have a higher usage
 rate.
\end_layout

\begin_layout Subsection
System fonts
\end_layout

\begin_layout Standard
Some of the difference between site screenshots and manually browsing to
 a site is in the fonts displayed.
 Most of the domains have been downloaded on a headless server, where fonts
 have not mattered to the system owner.
 Installing additional fonts commonly available on average user systems
 might reduce perceived difference.
\end_layout

\begin_layout Subsection
Do Not Track
\end_layout

\begin_layout Standard
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Write about Do Not Track and the response/server-side counter-part.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
While the HTTP header 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
DNT
\end_layout

\end_inset

 (Do Not Track) has not been set, it would have been interesting to look
 at the difference in response from remote services.
 As cookie headers can be analyzed, the difference could have been detected
 both per origin domain and per connected service.
 See also the P3P analysis 
\begin_inset CommandInset ref
LatexCommand eqref
reference "sub:Platform-for-Privacy-(P3P)-Project"

\end_inset

 for a related header.
\end_layout

\begin_layout Section
Domain lists
\end_layout

\begin_layout Standard
There are other domain lists that might have been suitable in this thesis.
 One curated top list is the KIA index, a list of top sites in Sweden aggregatin
g statistics from different sites' underlying analytics tools.
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "http://www.kiaindex.net/"

\end_inset


\end_layout

\end_inset

 Other TLDs, both from other countries and more generic ones, could be used.
\end_layout

\begin_layout Section
Ad and privacy blocking lists
\begin_inset CommandInset label
LatexCommand label
name "sec:Ad-and-privacy-blocking-lists"

\end_inset


\end_layout

\begin_layout Standard
There are several lists of known ads and privacy invading trackers in use
 in blocking software than Disconnect (see 
\begin_inset CommandInset ref
LatexCommand vref
reference "sub:Disconnect's-blocking-list"

\end_inset

).
 One of the most popular ones is EasyList 
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Insert link to EasyList, verify format.
\end_layout

\end_inset

, which exists in several varieties.
 They were considered, but in the end not incorporated because of the lower
 quality of the filter list format.
 It is a mixture of HTML element and URL blocking, and it lacks the connection
 between blocks and corresponding organization.
 There is also Ghostery, which uses a proprietary list that also contains
 organizations, but it has not been used because of licensing issues.
 
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Insert link to Ghostery, verify license.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
On a technical level, some blocking rule formats have also posed a problem
 in terms of implementation into the current data processing framework that
 is har-dulcify.
 It relies heavily on jq (see 
\begin_inset CommandInset ref
LatexCommand vref
reference "sub:jq"

\end_inset

), which does not have a public release that implements in regular expressions
 support, a major part of some blocking lists.
 The idea is that ads etcetera are filtered matching requests' complete
 URL against the blocking rules, which are a mixture of both more coarse
 and more fine-grained than Disconnect's domain based rules.
 One example is 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
/ads/
\end_layout

\end_inset

, matching a folder name that suggests that all URLs containing this particular
 path substring is serving advertisements.
 The thought of using a single path substring to block advertisements served
 from any domain is more coarse than pinpointing a single domain, but it
 is also more specific as it would not block legitimate content from another
 subfolder on the same domain.
 This way general ad serving systems can be blocked, while domains that
 serve both ads and content is still allowed serve the content without interferi
ng with the blocking of ads.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{futurework}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
At the time of writing, jq is released as version 1.4.
 Support for regular expressions is planned for version 1.5.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{futurework}
\end_layout

\end_inset


\end_layout

\begin_layout Section
Improving data analysis
\end_layout

\begin_layout Subsection
Automated testing
\end_layout

\begin_layout Standard
Data transformations have been written in a semi-structured manner, with
 separate files for most separate tasks, often executed in serial stages.
 Each task accepts a certain kind of data as input for the transformation
 to work correctly - but as both input and output from separate stages looks
 very similar, it is hard to tell which kind of data it accepts and what
 the expected output is - and if a change in one stage will affect later
 stages.
 Writing automated tests for each stage would have helped during both adding
 functionality and refactoring the structure.
 At times, there have been rather time-consuming problems with unexpected
 or illegal input from real world sites - extracting that kind of input
 to create a test suite would have sped up fixes and raised confidence in
 that the input would be handled appropriately and output would still be
 correct.
 So far that has not been done, and much of the opportunity to gain from
 tests have been lost as work has progressed past each problem.
\end_layout

\begin_layout Standard
One solution to validating both input and output would have been to create
 JSON schemas 
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Insert link to JSON schema.
\end_layout

\end_inset

 for each transformation.
 This kind of verification can easily be automated, and it will help any
 future changes.
\end_layout

\begin_layout Subsection
Code reuse
\end_layout

\begin_layout Standard
Much of the code written in shell scripts, both Bash and jq code, is duplicated
 between files.
 While common functionality suitable for pipelining have been broken out,
 shared functions have not.
 Bash provides the 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
source
\end_layout

\end_inset

 command for sharing functionality.
 Code sharing in jq through the use of modules and packages is still under
 development, but there is a way to load a single external command file.
 This file can be precomposed externally by concatenating files with function
 definitions first, and the actual usage of those functions second.
 The improvement was postponed due to the relative little reuse in early
 scripting and bright outlook on modules and packages support.
 As the number of scripts grew, code sharing/composition possibilities grew
 as well - and with them possible improvements in development speed, consistency
 and correctness.
 At this stage, software stability is more important for the final dataset
 download and analysis, and code refactoring can only be postponed.
 Foreseeing a greater reuse of JSON and jq tools, a separate open source
 project has been started - jq-hopkok 
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Insert link to jq-hopkok.
\end_layout

\end_inset

 - where some scripts have been collected.
 Many functions and utilities local to har-dulcify are project-agnostic,
 and thus suitable objects to move to jq-hopkok for ease composition.
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{futurework}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
At the time of writing, jq is released as version 1.4.
 Support for modules is planned for a version after 1.5.
 Packages/package managers are external to the jq core, and do not follow
 the same planning.
 
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Verify version planning.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{futurework}
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Ignore domains without content
\end_layout

\begin_layout Standard
Many domains do not contain any actual content.
 Examples include web server placeholder pages (
\begin_inset Quotes eld
\end_inset

Installation succeeded
\begin_inset Quotes erd
\end_inset

), domain listings (
\begin_inset Quotes eld
\end_inset

Index of /
\begin_inset Quotes erd
\end_inset

), parked domains (
\begin_inset Quotes eld
\end_inset

This domain was purchased from ...
\begin_inset Quotes erd
\end_inset

) and advertisement domains (like Google Adwords for Domains 
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Insert link.
\end_layout

\end_inset

).
 There is a Sweden-centric list of site titles for recognized non-content
 pages available internally at .SE, but it has not been incorporated.
\end_layout

\begin_layout Section
Other views on the same data
\end_layout

\begin_layout Subsection
Cookie syncing
\end_layout

\begin_layout Standard
A recent large-scale study
\begin_inset CommandInset citation
LatexCommand cite
key "G.-Acar:persistent:2014aa"

\end_inset

 included a cookie syncing privacy analysis.
 It was shown that unique user identifiers were shared between different
 third parties.
 IDs can be shared in different ways.
 If both third parties exist on the same page, they can be shared through
 scripts or by looking for any IDs in the location URL.
 They can also be shared by one third-party sending requests to a second
 third-party (a fourth-party?), either by leaking the location URL as a
 HTTP referrer or by embedding it in the request URL.
 In crawls of Alexa's top 3000 domains, one third-party script in particular
 sends requests with synced IDs to 25 domains; the IDs were eventually are
 shared with 43 domains.
 They also showed that a user's browsing history reconstruction rate rose
 from 1.4% to 11% when backend/server-to-server overlaps were modeled.
\end_layout

\begin_layout Standard
The study used a modified Firefox browser to look at values stored in primarily
 cookies.
 As all HTTP requests are recorded in this thesis, including HTTP cookie
 headers, a limited version of the same study could be performed.
 In addition, they look at in-browser scripting utilizing for example 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
localStorage
\end_layout

\end_inset

, 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
canvas
\end_layout

\end_inset

 fingerprinting and ID storage in external plugins like Flash.
 While that might be possible, the modifications that would need to be made
 to phantomjs are non-trivial, and my current scope does not allow for that.
 With their research as a base, cookie respawning and sharing could possibly
 be confirmed using this thesis' code as a external tool using a different
 browser platform.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Pages with wide tables start here.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{wide}
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Platform for Privacy Preferences (P3P) Project
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "http://www.w3.org/P3P/"

\end_inset


\end_layout

\end_inset

 HTTP header analysis
\begin_inset CommandInset label
LatexCommand label
name "sub:Platform-for-Privacy-(P3P)-Project"

\end_inset


\end_layout

\begin_layout Standard
P3P is a way for websites to declare their policies and intentions for data
 collected from web users.
 It is declared in a machine-readable format, as an XML file and in a compact
 encoding as a HTTP header.
 W3C's work started in 1997 and P3P 1.0 became a W3C recommendation in 2002.
 It never gained enough momentum and the work with P3P 1.1 was suspended
 in 2006.
 P3P is still implemented by many websites, even though it may not follow
 the originally intended usage.
\end_layout

\begin_layout Standard
In conversations with Dwight Hunter, privacy policy researcher, he mentioned
 that P3P policies are seen as a good technical solution to policy problems
 in research he had read.
 
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Add references to Dwight's claims?
\end_layout

\end_inset

 I countered by saying that there are policy-wise useless P3P headers being
 sent from some webpages, most probably to bypass Internet Explorer's (not
 all versions) strict cookie rules for third-party site without a P3P HTTP
 header.
 This was been highlighted by Microsoft in 2012, pointing at Google's P3P
 use.
\end_layout

\begin_layout Quotation
By default, IE blocks third-party cookies unless the site presents a P3P
 Compact Policy Statement indicating how the site will use the cookie and
 that the site’s use does not include tracking the user.
 Google’s P3P policy causes Internet Explorer to accept Google’s cookies
 even though the policy does not state Google’s intent.
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "http://blogs.msdn.com/b/ie/archive/2012/02/20/google-bypassing-user-privacy-settings.aspx"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Looking at collected HAR data there are many examples of P3P headers.
 In the dataset 
\begin_inset Quotes eld
\end_inset

se.2014-07-10.random.100000-http
\begin_inset Quotes erd
\end_inset

 from 2014-09-01 with about 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
numprint{1944000}
\end_layout

\end_inset

 recorded requests, about 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
numprint{90000}
\end_layout

\end_inset

 present a P3P policy.
 There are about 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
numprint{550}
\end_layout

\end_inset

 unique values, including these:
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
tsvtable{p3p.counts.tsv}{Top P3P values}{}{display columns/1/.style={string
 type, column type=l}}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Quotes eld
\end_inset

Potato
\begin_inset Quotes erd
\end_inset

 comes from an example in a discussion regarding Internet Explorer and cookie
 blocking.
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Foot
status open

\begin_layout Plain Layout

\size normal
\emph on
Cookie blocked/not saved in IFRAME in Internet Explorer
\family roman
\series medium
\shape up
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 
\begin_inset CommandInset href
LatexCommand href
target "http://stackoverflow.com/a/16475093"

\end_inset


\end_layout

\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 Other examples include 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
CP="This is not a P3P policy! It is used to bypass IEs problematic handling
 of cookies"
\end_layout

\end_inset

, 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
CP="This is not a P3P policy.
 Work on P3P has been suspended since 2006: http://www.w3.org/P3P/"
\end_layout

\end_inset

, 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
CP="This is not a P3P policy.
 P3P is outdated."
\end_layout

\end_inset

, 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
CP=
\backslash
"Thanks IE8
\backslash

\end_layout

\end_inset

 (malformed), 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
CP="No P3P policy because it has been deprecated"
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
This is but one example of where quantitive analysis of real-world web pages
 shows differences between technical, intended or perceived usage.
 While P3P may be an outdated example that has been researched
\begin_inset CommandInset citation
LatexCommand cite
key "CMU-CyLab-10-014"

\end_inset

, it shows how automated, generic tooling can help researchers a lot in
 their understanding of usage in the wild.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{wide}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Pages with wide tables end here.
\end_layout

\end_inset


\end_layout

\begin_layout Chapter
Time plan
\end_layout

\begin_layout Standard
The time plan has been scrapped, due to unplanned conferences, holidays
 and vacations.
\end_layout

\begin_layout Section
Completed activities and milestones
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
2014-01-14 Brainstorming a thesis subject with company supervisor Patrik
 Wallström.
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
2014-01-28 Brainstorming a thesis subject with examiner Niklas Carlsson.
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
2014-02-07 Finalized the subject proposal, submitted to examiner and company
 supervisor.
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
2014-03-07 Initial subject discussion meeting at .SE, with company supervisor
 Staffan Hagnell.
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
2014-03-18 First subject draft ready and sent to examiner, company supervisors
 and other interested parties.
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
2014-03-31 Subject draft approved by examiner.
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
2014-W15 Finalize planning report.
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
2014-W15 Start software development efforts.
\end_layout

\begin_layout Section
Planned activities and milestones
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
2014-W19 Half time evaluation.
 Have preliminary results ready, as a progress indicator.
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
2014-W23 Thesis draft for supervisors to review, then revise according to
 comments.
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
2014-W24 Thesis draft for the examiner to review, then revise according
 to comments.
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
2014-W25 Thesis draft for the opposition/peer review, then revise according
 to comments.
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
2014-W28 Thesis approval for presentation.
\end_layout

\begin_layout Section
Unplanned activities and milestones
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
Opponent/peer A student in the same field is required for the opponent/peer
 review.
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
Presentation The presentation date is required to be held during the regular
 semester period.
 The fall semester starts 2014-09-01.
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
Publication Thesis must be submitted to LiU E-Press after the presentation.
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
Evaluation Writing of an individual evaluation report, which is then discussed
 with the examiner and supervisors.
\end_layout

\begin_layout Chapter
Thanks
\end_layout

\begin_layout Standard
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
Add list of collaborators in getting data, domains, help, advice etcetera.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset nomencl_print
LatexCommand printnomenclature
set_width "auto"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "Content"
description "Information and data that is presented to the user. Includes text, images, video and sound."

\end_inset


\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "Resource"
description "An entity external to the HTML page that requested it. Types of resources include images, video, audio, CSS, javascript and flash animations."

\end_inset


\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "External resource"
description "A resource downloaded from a domain other than the page that requested it was served from."

\end_inset


\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "External service"
description "A third party service that delives some kind of resource to the user's browser. The service itself can vary from showing additional information and content, to ads and hidden trackers.\\\\\\\\External services include file hosting services, CDNs, advertisting networks, statistics and analytics collectors, and third party content."

\end_inset


\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "Tracker"
description "An resource external to the visited page, which upon access receives information about the user's system and the page that requested it.\\\\\\\\Basic information in the HTTP request to the resource URL includes user agent (browser vendor, type and version down to the patch level, operating system, sometimes hardware type) referer (the full URL of page that requested the resource), an etag (unique string identifying the data from a previous request to the same resource URL) and cookies (previously set by the tracker)."

\end_inset


\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "CDN"
description "Content delivery network"

\end_inset


\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "Content delivery network"
description "(CDN) The speed at which data can be delivered is dependant on distance between the user and the server. To reduce latency and download times, a content delivery network places multiple servers with the same content in strategic locations, both geographic and network toplolgy wise, closer to groups of users.\\\\\\\\For example, a CDN could deploy servers in Europe, the US and Australia, and reduce loading speed by setting up the system to automatically use the closest location."

\end_inset


\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "Domain name"
description "A domain name is a human-readable way to navigate to a service on the internet: example.com. Fully qualified domain names (FQDN) have at least two parts - the top level domain name (TLD) and the second-level domain name - but oftentimes more depending on TLD rules and organizational units.\\\\\\\\Domains are also used, for example, as logical entities in regards to security and privacy scopes on the web, often implemented as same-origin policies. As an example, HTTP cookies are bound to domain that set them."

\end_inset


\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "Third-party content"
description "Content served by another organization than the organization serving the explicitly requested web page. Also see external resource."

\end_inset


\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "Third-party service"
description "A service provided by an organization other than the explicitly requested service. Also see external service."

\end_inset


\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "Web site"
description "A collection of web pages under the same organization or topic. Often all web pages on a domain is considered a site, but a single domain can also contain multiple sites."

\end_inset


\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "Web service"
description "A function performed on the internet, and in this document specifically web sites with a specific purpose directed towards human users. This includes search engines, social networks, online messaging and email as well as content sites such as news sites and blogs."

\end_inset


\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "Web browser"
description "Or browser. Software a user utilizes to retrieve, present and traverse information from the web."

\end_inset


\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol ".SE"
description "The Internet Infrastructure Foundation. An independent organization for the benefit of the public that promotes the positive development of the internet in Sweden. .SE is responsible for the .se top level domain."

\end_inset


\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol ".se"
description "The country code top level domain name for Sweden."

\end_inset


\end_layout

\begin_layout Quote
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "../references/thesis"
options "bibtotoc,plain"

\end_inset


\end_layout

\end_body
\end_document
